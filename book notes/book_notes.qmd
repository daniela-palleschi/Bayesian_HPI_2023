---
title: "Chapter notes"
author: "Daniela Palleschi"
format:
  html:
    toc: true
    # html-math-method: katex
    css: styles.css
    number-sections: true
    number-depth: 3
    code-overflow: wrap
editor_options: 
  chunk_output_type: console
# bibliography: references.json
biblio-style: apalike
---

These notes are based on @nicenboim_introduction_nodate, both from a PDF version supplied by the authors and the html version available [here](https://vasishth.github.io/bayescogsci/book/) (accessed in early 2023). Much of the notes are taken verbatim from the book, as are code snippets.

::: {#refs custom-style="Bibliography"}
:::


```{r, eval = T, echo = F, message = F}
# Create references.json file based on the citations in this script
# rbbt::bbt_update_bib("book_notes.qmd")
```

# Set up{-}

```{r, results = "hide", warning=F,message=F,error=F}
# set global knit options
knitr::opts_chunk$set(echo = T, # print chunks?
                      eval = T, # run chunks?
                      error = F, # print errors?
                      warning = F, # print warnings?
                      message = F, # print messages?
                      cache = F # cache?; be careful with this!
                      )

# suppress scientific notation
options(scipen=999)

# play a sound if error encountered
options(error = function() {beepr::beep(9)})

# load packages
## create list of package names
packages <- c( #"SIN", # this package was removed from the CRAN repository
               "MASS", "dplyr", "tidyr", "purrr", "extraDistr", "ggplot2", "loo", "bridgesampling", "brms", "bayesplot", "tictoc", "hypr", "bcogsci", "papaja", "grid", "kableExtra", "gridExtra", "lme4", "cowplot", "pdftools", "cmdstanr", "rootSolve", "rstan"
  )

# NB: if you haven't already installed bcogsci through devtools, it won't be loaded
## Now load or install & load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# this is also required, taken from the textbook

## Save compiled models:
rstan_options(auto_write = FALSE)
## Parallelize the chains using all the cores:
options(mc.cores = parallel::detectCores())
# To solve some conflicts between packages
select <- dplyr::select
extract <- rstan::extract
```

# Ch. 1 - Intro

- given some data, how to use Bayesâ€™ theorem to ***quantify uncertainty about our belief*** regarding a scientific question of interest
- topics to be understood:
  - the basic concepts behind probability
  - the concept of random variables
  - probability distributions
  - the concept of likelihood

## Probability

Frequency-based versus uncertain-belief perspective of probability:

1. repeatable events, like rolling a die and getting a 6, are *frequentist* because **probability** is related to the *frequency* at which we'd observe an outcome given repeated observations
2. one-of-a-kind events, like earthquakes, don't work with this idea of probability
  - the probability of an earthquake expresses our *uncertainty* about an event happening
  - we also be *uncertain* about how probable an event is: being 90% sure something is 50% likely to happen
  - this is what we're interested in: how uncertain we are of an estimate
  
In Bayesian analysis, we want to express our uncertainty about the probability of observing an outcome (*prior distribution*).

### Conditional probability and Bayes' rule

- A = "the streets are wet"
- B = "it was raining"
- P(A|B) = the probability of A given B
- P(A,B) = P(A|B)P(B) (the probability of A and B happening)

### Law of total probability

- dunno

## Discrete random variables

Generating random sequences of simulated data with a binomial distribution. Imagine a cloze task, where we consider a particular word a success (1) and any other word a failure (0). If we run the experiment 20 times with a sample size of 10, the cloze probabilities for these 20 experiments would be:

```{r, results="asis"}
rbinom(10, n = 20, prob = .5)
```

For discrete random variables such as the binomial, the probability distribution *p(y|$\theta$)* is called a probability mass function (PMF) . The PMF defines the probability of each possible outcome. With *n* = 10 trials, there are 11 possible outcomes (0, 1, 2,...10 succeses). Which outcome is most probable depends on the parameter $\theta$ that represents the probability of success. Above, we set $\theta$ to `0.5`.

### The mean and variance of the binomial distribution

In real exerimental situations we never know the true value of $\theta$ (probability of an outcome), but it can be derived from the data: *$\theta$ hat = k/n*, where *k* = number of observed successess, *n* = number of trials, and *$\theta$ hat* = observed proportion of successes. *$\theta$ hat* = ***maximum likelihood estimate*** of the true but unknown parameter *$\theta$*. Basically, the **mean** of the binomial distribution. The **variance** can also be estimated by computing *(n($\theta$))(1 - $\theta$)*. These estimates can be be used for statistical inference.

### Compute probability of a particular outcome (discrete): dibinom

`dbinom` calculates probability of *k* successes out of *n* given a particular *$\theta$*.

```{r}
dbinom(5, size = 10, prob = .5)
dbinom(5, size = 10, prob = .1)
dbinom(5, size = 10, prob = .9)
```

With continuous data, the probability of obtaining an exact value will always be zero. We'll come ot this later.

### Compute cumulative probability: pbinom

The cumulative distribution function (CDF): essentially the sum of all probabilities of the values of *k* you are interested in. E.g., the probability of observing 2 successes or fewer (0, 1, or 2) is:

```{r}
# sum of probabilities for exact k's
dbinom(0, size = 10, prob = .5) +
  dbinom(1, size = 10, prob = .5) +
  dbinom(2, size = 10, prob = .5)

# or
sum(dbinom(0:2, size = 10, prob = .5))

# or use pbinom()
pbinom(2, size = 10, prob = 0.5, lower.tail = TRUE)
# conversely, what is the $\theta$ of observing THREE successes or more?
pbinom(2, size = 10, prob = 0.5, lower.tail = F)
# or
sum(dbinom(3:10, size = 10, prob = .5))

# the probability of observing 10 or fewer successes (out of 10 trials)
pbinom(10, size = 10, prob = 0.5, lower.tail = TRUE)
```

### Compute the inverse of the CDF (quantile function): qbinom

The quantile function (the inverse CDF) obtains the value of *k* (the quantile) given the probability of obtaining *k* or less than *k* successes given some specific probability value *p*:

```{r}
# reverse of dbinom(2,10,.5) would be:
qbinom(0.0546875, size=10, prob=.5)
```

#### Generage simulated data from binomial distribtion: rbinom

```{r}
# given 1 iteration of 10 trials where p = .5, produce a random value of k
rbinom(1, 10, .5)
```

## Continuous random variables

Imagine vector of reading times data with a normal distribution, defined by its *mean* and its *sd*. The ***probability density function*** (PDF) for particular values of mean and sd (assuming a normal distribution) can be calculated using `dnorm`. The CDF can be found using `pnorm`, and the inverse CDF using `qnorm`. These are 3 different ways of looking at the infrmation.

```{r}
# p of observing a mean of 250ms when the true mean is 500 & sd = 100 (PDF)
dnorm(400,mean = 500, sd = 100)

# p of observing 400ms *or lower* when the true mean is 500 & sd = 100 (CDF)
pnorm(400,mean = 500, sd = 100)

# k with a CDF of 0.1586553 when the true mean is 500 & sd = 100 (inverse CDF)
qnorm(0.1586553, mean = 500, sd = 100)
```

Question: what is the probability of observing values between 200 and 700 from a normal distribution where mean = 500 and sd = 100?

```{r}
pnorm(700,500,100) - pnorm(200,500,100)
```

With continuous data, it is only meaningful to ask about probabilities between two point values (e.g., probability that Y lies between a and b).

What is the quantile *q* such that the probability of observing that value or something less (or more) than it is 0.975 (given the normal(500,100) distribution)?

```{r}
qnorm(0.975, m=500, sd=100)
```

Next task: generate simulated data. generate 10 data points using the `rnorm` function and use this simulated data to compute the mean and stanrdard devaition.

```{r}
x <- rnorm(10,500,100)
mean(x)
sd(x)

# can also computer lower and upper bounds of 95% CIs
quantile(x, probs = c(.025, .975))
```

### An important distinction: probability vs. densitiy in continuous random variables

The probability density function (PDF):
```{r}
# density with default m = 0 and sd = 1
dnorm(1)
```

This is not the probability of observing 1 in this distribution, as the probability of a single value in a continous distribtion will always be 0. This is becaue probability in a continuous distritubion is the ***area under the curve***, and at a single point there is no area under the curve (i.e., p = 0). The `pnorm` function allows us to find the cumulative distribution function (CDF) for the normal distribution.

For example, the probability of obseving a value etween +/-2 in a normal distribution with mean 0 and sd 1:
```{r}
pnorm(2, m = 0, sd = 1) - pnorm(-2, m = 0, sd = 1)
```

For ***discrete*** random variables, the situation is different. These have a probability **mass** function (PMF), the binomial distribution that we saw before. Here, the PMF maps the possible *y* values to the probabilities of those exact values occurring.

```{r}
dbinom(2,size=10,prob=.5)
```

### Truncating a normal distribution

Refers to positive values only (truncating at 0).

## Bivariate and multivariate distributions

Consider a case where two discrete responses were recorded: a binary yes/no response, and a Likert acceptability rating (1-7).

The ***joint probability mass function*** is the joint PMF of two random variables.

Let's play around with some such data:
```{r}
# run if package is not loaded
# library(bcogsci)
data("df_discreteagrmt")
```

#### Marginal distributions

The marginal distribution of each pair of values (let's say *x* = the binary response, *y* = the Likert response) is computed by summing up 

```{r, eval = F}
rowSums(probs)
```

***object `probs` is not defined in the book***

### Generate simulated bivariate (multivariate) data

Suppose we want to generate 100 pairs of correlated data, with correlation rho = 0.6. The two random variables have mean 0, and standard deviations 5 and 10 respectively.

```{r}
## define a variance-covariance matrix:
Sigma <- matrix(c(5^2, 5 * 10 * .6, 5 * 10 * .6, 10^2),
  byrow = FALSE, ncol = 2
)
## generate data:
u <- mvrnorm(
  n = 100,
  mu = c(0, 0),
  Sigma = Sigma
)
head(u, n = 3)
```

```{r}
# plot the data
ggplot(tibble(u_1 = u[, 1], u_2 = u[, 2]), aes(u_1, u_2)) +
  geom_point()
```

## An important concept: the marginal likelihood (integrating out a parameter)

## Exercises

1.1 Practice with pnorm Part 1

Given a normal distribution with mean 500 and standard deviation 100, use the pnorm function to calculate the probability of obtaining values between 200 and 800 from this distribution.

```{r}
pnorm(800, mean = 500, sd = 100) - pnorm(200, mean = 500, sd = 100)
```

1.2 Practice with pnorm Part 2

```{r}
pnorm(700, 800, 150, lower.tail=T)
pnorm(900, 800, 150, lower.tail=F)
pnorm(800, 800, 150, lower.tail=F)
```

1.3 Practice with pnorm Part 3

```{r}
pnorm(550,600,200,lower.tail=T)
pnorm(800,600,200,lower.tail=T) -
  pnorm(300,600,200,lower.tail=T)
pnorm(900,600,200,lower.tail=F)
```

Exercise 1.4 Practice using the qnorm function - Part 1

```{r}
qnorm(c(.1,.9),mean=1,sd=1)
```

Exercise 1.5 Practice using the qnorm function - Part 2

```{r}
qnorm(c(.1,.9), mean=650, sd=125)
```

Exercise 1.6 Practice getting summaries from samples - Part 1

```{r}
data_gen1 <- rnorm(1000, 300, 200)

# mean
mean(data_gen1)
# sd
sd(data_gen1)

# q1 and q2
qnorm(c(.1,.9), mean(data_gen1), sd(data_gen1))

hist(data_gen1)
```

Exercise 1.7 Practice getting summaries from samples - Part 2

```{r}
# generate data with truncated normal distribution
data_gen1 <- rtnorm(1000, 300, 200, a = 0)

# mean
mean(data_gen1)
# sd
sd(data_gen1)

# q1 and q2
qnorm(c(.1,.9), mean(data_gen1), sd(data_gen1))

hist(data_gen1)
```

Exercise 1.8 Practice with a variance-covariance matrix for a bivariate distribution

# Ch. 2 - Intro to Bayesian data analysis

- simple but crucial point: the posterior distribution of a parameter is a compromise between the prior and the likelihood

## Terms {-}

- *posterior*, *p(*\$theta$*|y)*: probability distribution of the parameters conditional on the **data**
- *likelihood*, *p(y|*\$theta$*)*: the PMF or PDF expressed as a function of $\theta$
- *prior*, \$theta$: the initial probability distribution of paramters ***before seeing the data***
- *marginal likelihood, p(y)*: standardizes the posterior distribution to ensure the AUC sums to 1; it ensure the posterior is a valid probability distribution

## Bayes' Rule

- Bayes' rule: when *A* and *B* are observable discrete events (like "it has been raining" or "the streets are wet"), we can state the rule as follows:

$$
P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)}
\tag{2.1}
$$

- given a vector of data *y*, we can work out the posterior distributions of parameters of interest which we represent as the vector of parameters $\theta$
- to do this, we can re-write equation 2.1 as 2.2:

$$
p(\boldsymbol{\Theta}|\boldsymbol{y}) = \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta}) \cdot p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})}
\tag{2.2}
$$

- now, Bayes' rule is writen in terms of probability distributions, where p() is the probability density function (continuous) or probability mass function (discrete)
- in words, this simply means:

$$
\hbox{Posterior} = \frac{\hbox{Likelihood} \cdot \hbox{Prior}}{\hbox{Marginal Likelihood}}
$$

## Deriving the posterior using Bayes' Rule: an analystical example

- participants are shown sentences like *It's raining. I'm going to take the...*

- if 100 participants complete the sentence, and 80 complete the sentence with *bus*, the estimated cloze probability would be $\frac{80}{100}$=0.8
   - this is the **maximum likelihood estimate** of the probability of producing the word; as this is an estimate let's add a hat: $\hat \theta$=0.8 
   
- in the frequentist paradigm, $\hat \theta$=0.8 is an estimate of an **unknown point value** $\theta$ "out there in nature"

- N.B., the variability in the estimate will be influenced by the sample size
  - if the *true* value of $\theta$ is really 0.80, we will still get some variability in the estimated proportion from a sample size of say 10 pariticpants
  - let's carry out 100 simulated experiments and compute their variability:
  
```{r}
estimated_means <- rbinom(n = 100, # generate 100 random binomial data sets
                          size = 10, # of 10 obvs each
                          prob = .8) / 10 # with prob 8, now divide these by 10 to get 100 means (k/n)

# what is the sd of these 100 means?
round(sd(estimated_means),3)
```
  
- instead, let's imagine that $\theta$ is a ***random variable***; i.e., it has a PDF associated with it
  - this PDDF would now represent our ***belief*** about possible values of $\theta$ *before we have any data*
  - e.g., if we believe from the outset that all possible values between 0 and 1 are equally likely, we would have a uniform prior of $\theta \sim \mathit{Uniform}$(0,1)
  - let's re-run our simulated experiments, but with *two* sources of variability: the data and our uncertainty associated with $\theta$
  
```{r}
theta <- runif(100, min = 0, max = 1) # simulate 100 numbers between 0:1

estimated_means <- rbinom( # generate random binomial data that has...
  n = 100, # 100x
  size = 10, # of 10 obvs
  prob = theta # with prob = theta
)/10 # divided by 10 to give us the mean for each

# sd of the means from these 100 'experiments'
round(sd(estimated_means),3)
```

- the higher standard deviation, representing variability int he estimate of the parameter, comes frm the added uncertainty from the $\theta$ parameter
  - what would happen if we had tighter expectations, i.e., a very tight PDF for $\theta$, say \theta \sim \mathit{Uniform}(0.7,0.9)?
  
```{r}
theta <- runif(100, min = 0.7, max = 0.9) # simulate 100 numbers between 0:1

estimated_means <- rbinom( # generate random binomial data that has...
  n = 100, # 100x
  size = 10, # of 10 obvs
  prob = theta # with prob = theta
)/10 # divided by 10 to give us the mean for each

# sd of the means from these 100 'experiments'
round(sd(estimated_means),3)
```
  
- the variability is smaller; so the greater the uncertainty associated with the $\theta$ parameter, the greater the variability in the data

- this is very different from the frequentist assumption that $\theta$ is a point value; in Bayesian $\theta$ is a **random variable** with a probability density/mass function associated with it
- this PDF is called a **prior distribution** and represents our prior belief or knowledge about a possible value of this parameter
- once we obtain data, these data serve to modify our prior belief about the distribution, called our **posterior distribution**

## Choosing a likelihood

- with a binomial distribution like the cloze probability (chose 'bus' or not), the PMF can be written as:

$$
p(k|n,\theta) = \binom{n}
{k} \theta^k (1-\theta)^{n-k}
\tag{2.3}
$$

- *k* = the number of times "bus" was given as an answer
- *n* = the total number of answers given
- if we collect 100 data points (*n* = 100), and find *k* = 80, we now have 2 fixed data points, *n* and *k*. The only variable is now $\theta$

$$
p(k=80 | n= 100,  \theta) = \binom{n}{k} \theta^{80} (1-\theta)^{20}
$$

- this is now a continuous function of the value of $\theta$, which can have a possible value between 0 and 1
- by contrast, the PMF of the binomial treats $\theta$ as a **fixed** value and defines a discrete distribution over the n+1 possible discrete values *k* that we can observe

- recall: the PMF and the likelihood are the same function seen from different points of view: the only difference being what is considered fixed (PMF: $\theta$, likelihood: data) and what is varying (PMF: data, likelihood: $\theta$)
  - **PMF**: $\theta$ is fixed, data varies
  - **likelihood function**: data is fixed, $\theta$ varies

Now we go back to our main foal: using Bayes' rule to find out the posterior distribution of $\theta$ given our data: *p(*$\theta$*|n,k). We first need to define a prior distribution over the parameter $theta$, thereby expressing our prior uncertainty about plausible values of $\theta$
  
## Choosinga a prior for $\theta$

- priors for a $\theta$ in a binominal distribution: the parameter $\theta$ is a random variable with a PDF whose range is [0,1]

- the ***beta distribution***, which is a PDF for a continuous random variable, is commonly used as a prior for parameters representing probabilities, and has the following PDF:

$$
p(\theta|a,b)=  \frac{1}{B(a,b)} \theta^{a - 1} (1-\theta)^{b-1}   
\tag{2.4}
$$

- *B(a,b)* is a normalising constant that ensures that the area under the curve sums to 1, so that *p(*$\theta$|*a,b)* is a probability
- the beta distribution's paramters *a* and *b* express our prior beliefs about the probability of a success:
  - *a* = number of "successes" (answering "bus")
  - *b* = number of "failures" (not answering "bus")
- the different beta distributions shapes given different values of a and b are shown below (in r, a = `shape1` and b = `shape2`)

```{r}
plot(function(x) 
  dbeta(x,shape1=1,shape2=1), 0,1,
      main = "Beta density",
  ylab="density",xlab="theta",ylim=c(0,3))

text(.5,1.1,"a=1,b=1")

plot(function(x) 
  dbeta(x,shape1=3,shape2=3),0,1,add=TRUE)
text(.5,1.6,"a=3,b=3")


plot(function(x) 
  dbeta(x,shape1=6,shape2=6),0,1,add=TRUE)
text(.5,2.8,"a=6,b=6")

plot(function(x) 
  dbeta(x,shape1=2,shape2=6),0,1,add=TRUE)
text(.15,2.9,"a=2,b=6")

plot(function(x) 
  dbeta(x,shape1=6,shape2=2),0,1,add=TRUE)
text(.85,2.9,"a=6,b=2")
```

- to express our uncertainty, we could compute 95\% ***credible*** intervales, i.e., the region over which we are 95\% certain the value of the parameter lies

```{r}
# compute 95% CrIs
round(
  qbeta(# at what quantiles (points on the x-axis)
  c(.025,.975), # would the PDF cover 95% AUC
  shape1 = 4, # where a = 4
  shape2 = 4), # and b = 4
  3) # rounded to 3 decimal points
```

:::{.callout-tip}
## Sidebar
- in a unimodal distribution, one could use the narrowest interval that contains the mode (the **highest posterior density interval** (HDI))
- in skewed posterior distirbutions, the equal-tailed CrI and the HDI will not be identical, because the HDI will have unequal tail probabilities; this book uses the equal-tailed interval (like we computed above) because it's the standard output in `Stan` and `brms`
:::

- if we were to choose *a* = 10 and *b* = 10, we would still be assuming a prior that "bus" is just as likely as some other word, but now our prior uncertainty about this mean is lower, meaning we have a tighter prior:

```{r}
# compute 95% CrIs
round(
  qbeta(# at what quantiles (points on the x-axis)
  c(.025,.975), # would the PDF cover 95% AUC
  shape1 = 10, # where a = 4
  shape2 = 10), # and b = 4
  3) # rounded to 3 decimal points
```

- compare the AUC for the two beta distributions in @fig-beta2

```{r fig-beta2}
#| fig-cap: "Beta distributions with varying a and b values"

plot(function(x) 
  dbeta(x,shape1=4,shape2=4), 0,1,
      main = "Beta density",
  ylab="density",xlab="theta",ylim=c(0,4))
text(.5,2.35,"a=4,b=4")

plot(function(x) 
  dbeta(x,shape1=10,shape2=10),0,1,add=TRUE,
  ylab="density",xlab="theta",ylim=c(0,4))
text(.5,3.7,"a=10,b=10")
```

- but which prior should we choose? This depends on our prior knowledge
  - if we don't have much prior information, we could use *a* = *b* = 1; this is a uniform prior $\mathit{Uniform}$(0,1), often clled a ***flat***, ***non-informative***, or ***uninformative prior***
  - if we have a lot of prior knowledge or a strong belief regarding the range of plausible values for $\theta$, we can use a different set of *a* and *b* values
  - if we were to use *a* = 4 and *b* = 4, then our prior for $\theta$ would be:

$$
p(\theta) = \frac{1}{B(4,4)} \theta^{3} (1-\theta)^{3}
$$


## Using Baye's rule to computer the posterior *p(*$\theta$*|n,k)*

- recall the equation from earlier:

$$
\hbox{Posterior} = \frac{\hbox{Likelihood} \cdot \hbox{Prior}}{\hbox{Marginal Likelihood}}
$$

- now that we've got the likelihood and the prior, we can use Bayes' rule to calculate *p(*$\theta$|*n,k)*, as follows:

$$
p(\theta|n=100,k=80) = \frac{\left[\binom{100}{80} \theta^{80} \cdot (1-\theta)^{20}\right]  \times \left[\frac{1}{B(4,4)} \times \theta^{3} (1-\theta)^{3}\right]}{p(k=80)}
\tag{2.6}
$$

- constant values are those that do not depend on the unknown parameter of interest, $\theta$; so *p(k = 80)* will be a constant once we know the number of successes
  - once *k* is known, we already have several constant values

$$
p(\theta|n=100,k=80) =   \left[ \frac{\binom{100}{80}}{B(4,4)\times p(k=80)} \right]   [\theta^{80} (1-\theta)^{20} \times  \theta^{3} (1-\theta)^{3}]
\tag{2.7}
$$

- we can gather all the constants (in the square brackets below):

$$
p(\theta|n=100,k=80) =   \left[ \frac{\binom{100}{80}}{B(4,4)\times p(k=80)} \right]   [\theta^{80} (1-\theta)^{20} \times  \theta^{3} (1-\theta)^{3}]
\tag{2.7}
$$

- and ignore the constants for now, which will later on make the AUC sum up to 1; so now we say the posterior is proportional to the right-hand side of the equation:

$$
p(\theta|n=100,k=80) \propto   [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3} ]
\tag{2.8}
$$

- in other words:

$$
\hbox{Posterior} \propto \hbox{Likelihood} \times \hbox{Prior}
$$

- now we just have to add up the exponents

$$
p(\theta|n=100,k=80) \propto   [\theta^{80+3} (1-\theta)^{20+3}] = \theta^{83} (1-\theta)^{23}
\tag{2.9}
$$

- recall that the beta distribution involves $\theta$ exponentiated to the power of *a*-1 and *b*-1 (see equation 2.4)
  - therefore, the expression in 2.9 above corresponds to a beta distribution with parameters *a* = 84 and *b* = 24 (because 83 and 23 +1 is 84 and 24)
  - all we need now is our  normalising constant to make the AUC sum to one; let's check this:
  
```{r}
PostFun <- function(theta) {
  theta^83 * (1 - theta)^23
}
(AUC <- integrate(PostFun, lower = 0, upper = 1)$value)
```

- this doesn't add up to 1, it's not a probability distribution
- but we can use it to figure out what our normalising constant is; what is the constant *k* such that the AUC sums to 1:

$$
k \int_{0}^{1} \theta^{83} (1-\theta)^{23} = 1
$$

- we know what \int_{0}^{1} \theta^{83} (1-\theta)^{23} is, because we just computed it above (and called it AUC), so:

$$
k  = \frac{1}{\int_{0}^{1} \theta^{83} (1-\theta)^{23}} = \frac{1}{AUC}
$$

- we now have the distribution or $\theta$ given the data, expressed as a PDF:

$$
p(\theta|n=100,k=80) = \frac{1}{B(83,23)} \theta^{84-1} (1-\theta)^{24-1} 
$$

- and our function will now sum to one if we divide it all by AUC

```{r}
PostFun <- function(theta) {
  theta^83 * (1 - theta)^23 / AUC
}
integrate(PostFun, lower = 0, upper = 1)$value
```

## Summary of the procedure

- we started with a binomial likelihood
  - multiplied it with the prior $\theta \sim \mathit{Beta}(4,4)$
  - obtained the posterior *p(*$\theta$|*n,k)* $\sim \mathit{Beta}(4,4)$
  - we ignored the constants when carrying out the multiplication (i.e., computer the posterior *up to proportionality*)
  - then we rescaled the posterior to become a probability distribution by including a proportionality constant (AUC)
  
- this was an example of a **conjugate** analysis: the posterior on the parameter has thes ame form (belongs to the same family of probability distributions) as the prior
- this combo of likelihood and prior is called the ***beta-binomial conjugate case***
  - conjugacy is defined as: Given the likelihood *p(y*|$\theta$*)*, if the prior *p(*$\theta$*)( results in a posterior $p(\theta|y)$ that has some form as $p(\theta)$, then we call $p(\theta)$ a conjugate prior
  
  - given a $Binomial(n,k|\theta)$ likelihood, and a $Beta(a,b)$ prior on $\theta$, the posterior will be $Beta(a + k, b + n - k)$

## Visualising the prior, likelihood, and the posterior

```{r}
k <- 80
n <- 100
## Prior
a <- 4
b <- 4
binom_lh <- function(theta) {
dbinom(x=k, size =n, prob = theta)
}
K <- integrate(f = binom_lh, lower = 0, upper = 1)$value
binom_scaled_lh <- function(theta) 1/K * binom_lh(theta)
  
p_beta <- ggplot(data = tibble(theta = c(0, 1)), aes(theta)) +
  stat_function(
    fun = dbeta,
    args = list(shape1 = a, shape2 = b),
    aes(linetype = "Prior")
  ) +
  ylab("density") +
  stat_function(
    fun = dbeta,
    args = list(shape1 = k + a, shape2 = n - k + b), aes(linetype = "Posterior")
  ) +
  stat_function(
    fun = binom_scaled_lh,
    aes(linetype = "Scaled likelihood")
  ) +
  theme_bw() +
  theme(legend.title = element_blank())
p_beta
```

- if we wanted to produce the 95\% credible interval, i.e., the range over which we are 95\% certain the true value of $\theta$ lies, give *a* = 84 and *b* = 24:

```{r}
round(
  qbeta(c(0.025, 0.975), shape1 = 84, shape2 = 24)
  ,3)
```

## The posterior distribution is ac ompromise between the prior and the likelihood

- let's take four different beta priors with increasing certainty:
  - $Beta(a = 2, b = 2)$
  - $Beta(a = 3, b = 3)$
  - $Beta(a = 6, b = 6)$
  - $Beta(a = 21, b = 21)$

- each reflects a believe that $\theta$ = 0.5 but with a varying degree of certainty
  - we can now "just" plug in the likelihood and the prior to the beta-binomial case to get the posterior:
  
$$
p(\theta | n,k) \propto p(k |n,\theta) p(\theta)
$$

- if we plot the tightest case ($a = 21, b = 21$), we see how the posterior is affected

```{r}
k <- 80
n <- 100
## Prior
a <- 21
b <- 21
binom_lh <- function(theta) {
dbinom(x=k, size =n, prob = theta)
}
K <- integrate(f = binom_lh, lower = 0, upper = 1)$value
binom_scaled_lh <- function(theta) 1/K * binom_lh(theta)
  
p_beta <- ggplot(data = tibble(theta = c(0, 1)), aes(theta)) +
  stat_function(
    fun = dbeta,
    args = list(shape1 = a, shape2 = b),
    aes(linetype = "Prior")
  ) +
  ylab("density") +
  stat_function(
    fun = dbeta,
    args = list(shape1 = k + a, shape2 = n - k + b), aes(linetype = "Posterior")
  ) +
  stat_function(
    fun = binom_scaled_lh,
    aes(linetype = "Scaled likelihood")
  ) +
  theme_bw() +
  theme(legend.title = element_blank())
p_beta
```

- we can say the following about the likelihood-prior-posterior relationship:
  - the posterior distribution is ac ompromise between the prior and the likelihood
  - for a given set of data, the great the certainty in the prior, the more heavily the posterior will be influenced by the prior mean
  - conversely, for a given set of data, the greater the **un**certainty in the prior, the more heavily the posterior will be influenced by the likelihood
  - but because *n* and *k* are included in the posterior Beta distribution ($Beta(a + k, b + n - k)$), the posterior mean will be influenced more heavily by larger sample sizes
    - sensitivity analyses can help check whether your parameter of interest is sensitive to the prior specification

## Incremental knowledge gain using prior knowledge

- we can incrementally gain information about a research question by using information from previous studies and deriving a position, and then using that posterior as a prior for the next experiment

- e.g., in the example above, we currently had a prior $Beta(4,4)$ and observed $k = 80$ successes of $n = 100$ observations, deriving a posterior $Beta(84,24)$
  - if we were to run this experiment again and had $k = 60, n = 100$, we cwould have a posterior as follows $\mathit{Beta}(a+k,b+n-k) = \mathit{Beta}(84+60,24+100-60)=\mathit{Beta}(144,64)$
  - alternatively, if we collected all this data in the first place and had a prior $Beta(4,4)$ and for data $k = 140, n = 200$, we'd have the same posterior: $\mathit{Beta}(4+140,4+200-140)=\mathit{Beta}(144,64)$

- so, we can keep building on our previous findings to inform future priors

## Summary

- we'll continue down the same path we followed in this chapter moving forward:
  1. decide on an appropriate likelihood function
  2. decide on prior for all the parameters involved in the likelihood function
  3. using this model (the likelihood and the priors) derive the posterior distribution of each parameter
  4. draw inferences about our research question basedon the posterior distribution of the parameter
  
# Ch. 3 - Computational Bayesian data analysis

- for real datasets, it was too cumbersome to do all the math to dertermine posterior distributions
  + thanks to probabilistic programming languages, we can define our models without have to do all the math
  
## Deriving the posterior through sampling

- recall the example cloze task for *It's raining, I'm going to take the...*, with the 'correct' answer *bus* ('umbrella' in the book but to me 'bus' is the most natural completion)
  + imagine 80 'successes' and 20 'failures'
  + assuming a binomial distribution as the likelihood function, and $Beta(a = 4, b = 4)$ as a prior distribution for the cloze probability
  + if we can obtain samples from the posterior distribution or $\theta$, instead of an analystically derived posterior distribution, given enough samples we will have a good *approximation* of the posterior distribution
  + '*obtain samples*' here means a situation similar to when we use `rbinom` or `rnorm` to obtain samples from a particular distribution
  + assume we used some probabilistic prgramming langauge to obtain 20,000 samples from the posterior distribution of the cloze probability $\theta$

## Bayesian regression models using Stan: brms

- because of increased computing power and probabilistic programming languages (e.g., WinBUGS, JAGS, R-INLA, pymc3, Turing, Stan), Bayesian statistics is now more popular
  + these languages allow th euser to define models without the complexities of the sampling process
  + however, they require learning a new language as te statistical model must be specified using a specific syntax
  + additionally, some knowledge of the *sampling process* is needed to correctly parametrize the models and avoid convergence issues

- Bayesian inference in `R` is possible without having the fully specify the model thanks to `stanarm` and `brms` packages
  + both packages provide Bayesian equivalents of R model-fitting functions like `(g)lmer`
  + both use Stan as the back-end for estimation and sampling

- for this part of the book we will focus on `brms`
  + it can be useful for a smooth transition from frequentist models to their Bayesian equivalents
  + it has the added benefit that the Stan code can be inspected via `brms::make_stancode()` and `brms::make_standata()`
  + users can then customatize their models or learn from the code produced internally by `brms`
  
### A simple linear model: A single subject pressing a button repeatedly

- imagine having data from a single participant repeatedly pressing the spacebar as fast as possible
  + the data are response times in imilliseconds in each trial; we want to know how long it takes to press a key for this subject
  
- let's model the data with thef ollowing assumptions:
  1. Tehre is a true (unknown) underlying time, $\mu$ ms, that the subject needs to press the psace bar
  2. There is some noise in this process
  3. The noise is normally distributed (this assumption is questionable given that response times are generally skewed, we will fix this assumption later)

This means that the likelihood for each observation $n$ will be:

$$
rt_{n} \sim Normal(\mu, \sigma)
\tag{3.2}
$$

- where $n$ = 1, ..., $N$, and $rt$ is the dependent variable (RTs in ms)
  + the variable $N$ indexes the total number of data points
  + $\mu$ indicates the *location* of the normal distirbution function; the lcoation parameter shifts the distribution left or right on the horizontal axis
  + in the *normal distribution*, the location is also the mean of the distribution
  + $\sigma$ indicates the *scale* of the distribution; as the scale decreases, the distribution gets narrower
  + for the normal distribution, the scale is also the standard deviation
  
- this same equation can be expressed as:

$$
rt_n = \mu + \varepsilon \hbox{, where } \varepsilon_n \stackrel{iid}{\sim} \mathit{Normal}(0,\sigma) \tag{3.3} 
$$

- this version of the model should be understood to mean that each data point $rt_n$ has some variability around a mean value $\mu$, and that variability has standard deviation $\sigma$
  + the term $iid$ ('independent and identically distributed') implies that each data point $rt_n$ is independently generated (i.e., not correlated with any of the other data points), and is coming from the same distribution ($Normal(\mu,\sigma)$)
  
- **Frequentist model**: that will give us the *maximu likelihood estimate* (the sample mean) of the time it takes to press the space bar
  + this owuld be enough ifnmroamtion to write the formular in `R`, `lm(rt ~ 1)`

- **Bayesian linear model**: we will also need to define *priors* for the two parameters of our model
  + let's say we know for sure that the time it takes to press a key will be positive and lower than a minute (0-60,000ms), but we don't want to make a commitment regarding which values are more likely
  + we encode what we know about the noise in the task in $\sigma$: this parameter must be positive and we'll assume any value below 2000ms is equally likely; such *flat* or *uniformative* priors are generaly strongly discouraged: it will almost never be the best approximation of what we know
  + let's start with such priors, regardless:
  
$$
\begin{aligned}
\mu &\sim \mathit{Uniform}(0, 60000) \\
\sigma &\sim \mathit{Uniform}(0, 2000) 
\end{aligned}
\tag{3.4}
$$

- load the data from the `bcogsci` package

```{r}
data("df_spacebar")
head(df_spacebar)
```

- plot the data before you do anything else; as we suspected, the data lock a bit (positively) skewed, but let's ignore that for now

```{r}
#| fig-width: 6
#| fig-align: center

df_spacebar %>%
  ggplot(aes(rt)) +
  labs(title = "Button-press data",
       x = "response times") +
  geom_density() +
  theme_bw()
```

#### Specifying the model in `brms`

- fit the model defined by equations \ref{3.2} and \ref{3.4}

```{r}
fit_press <- brm(
  rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(
      uniform(0, 60000),
      class = Intercept, # mean
      lb = 0,
      ub = 60000
    ),
    prior(
      uniform(0, 2000),
      class = sigma, # sd
      lb = 0,
      ub = 2000
    )
  ),
  chains = 4,
  iter = 2000,
  warmup = 1000
)
```

- some differences between this syntax and `lm()`:
  1. `family = gaussian()` makes it explicity that the underlying likelihood function is a normal distribution
    + this is implicit in `lm()`
    + the default for `brms` is `gaussian()`
    + other linking function are possible, just like in the `glm()` function
  2. `prior` takes as argument a vector of priors
    + this is optional, but we should ***always*** explicitly specify each prior; otherwise `brms` will define priors but they may or may not be appropriate
    + this is why we need `lb` (lower bound) and `upper bound` to specify the plausible range of values to sample from in cases where the distribution is restricted (e.g., reaction times cannot be negative, so `lb` must be at least 0)
  3. `chains` refers to the number of independent runs for sampling
    + default = 4
  4. `iter` refers to the number of iteratiosn that a sampler makes to sample from the posterior distribution of each paramter
    + default = 2000
  5. `warmup` refers to the number of iterations from the start of sampling that are eventually discarded
    + default = $\frac{`iter`}{2}$

- the last 3 options determine the behaviour of the sampling algorithm

#### Sampling and convergence in a nutshell

- our 4 chains start independently from each other
  + each chain "searches" for samples of the posterior distribution in a multidimensional space, where each parameter corresponds to a dimension
  + the shape of this space is determined by the priors and likelihood
  + chains start at a random location and each iteraton takes one sample each
  + when sampling begins, the samples may or may not belong to the posterior distributions of the parameters; eventually the chains end up in the vicinity of the posterior and from then on the samples will belong to the posterior
  
- therefore, when sampling starts the samples from the different chains can be far from each other; at some point they will **converge** and start delivering samples from the posterior distributions
  + typically the default values of `brms` will be sufficient to achieve convergence
  + if not, `brms` (but really `Stan`) will print out warnings with suggestions for fixin the convergence problems
  + this is why we remove the `warmup` samples, because the chains can start far apart and not in the posterior distribution
  + so, if we run 4 chains with 2000 iterations, we will obtain a total of 4000 iterations ($\frac{4 chains * 2000 iterations}{2} = \frac{8000}{2}$)

#### Output of `brms`

- once the model has ben fit (and assuming we didn't get any warning messages about convergence), we can print out the samples of the posterior distributions using `as_draws_df()`

```{r}
as_draws_df(fit_press) %>%
  head(3)
```

- `b_Intercept` corresponds to our $\mu$; we can ignore the last 2 columns
- plot the density and trace plot of each paramter after warmup:

```{r}
plot(fit_press)
```

- and print the object with the brms fit

```{r}
fit_press
```

- or with `posterior_summary()`

```{r}
posterior_summary(fit_press)
```

- `Estimate` is just the *mean* of the posterior samples
- `Est.Error` is the *standard deviation* of the posterior
- `CI`s mark the upper and lower bounds of the 95\% *credible intervals*

```{r}
as_draws_df(fit_press)$b_Intercept %>% 
  mean()
```

```{r}
as_draws_df(fit_press)$b_Intercept %>% 
  sd()
```

```{r}
as_draws_df(fit_press)$b_Intercept %>%
  quantile(c(.025,.975))
```

- summary also provides:
  + `Rhat`: compares between- and within-chain estimate of each parameter
    + is >1 when chains have not mixed well; we can only rely on the model if the R-hats for *all* parameters are <1.05 (warnings will appear otherwise)
  + `Bulk_ESS`: 'bulk effective sample size' is a measure of sampling efficienty in the bulk of the posterior distribution
    + i.e., the effectice sample size for the mean and median estimates
  + `Tail_ESS`: 'tail effective sample size': the sampling efficiency at the tails of the distribution
    + i.e., the minimum of effective sample sizes for 5\% and 95\% quantiles
  + the number of post-warmup samples is generally lower than the effective sample size, because the samples from the chains are not independent (they are correlated to some extent)
, and carry less information about the posterior distribution in comparison to *independent* samples
- very low sample size indicates sampling problems (and will produce warnings) and in general appear when chains are not properly mixed
  + as a rule of thumb, a minimum of 400 effective sample size is required for statistical summaries
  
- we wee our model fits without problems, and we get some posterior distribution for our parameters, but we should ask the following questions:

1. What informationa re the priors encoding? Do the priors make sense?
2. Does the likelihood assumed int he model make sense for the data?

- to answer these questions we can look at the *prior* and *posterior distributions* and we can do sensitivity analyses

## Prior predictive distribution

- we had the following priors in our linear model:

$$
\begin{aligned}
\mu &\sim \mathit{Uniform}(0, 60000) \\
\sigma &\sim \mathit{Uniform}(0, 2000) 
\end{aligned}
\tag{3.5}
$$

- these priors encode assumptions about our data
- to understand these assumptions, we are going to generate data from the model
  + such data, which is generated entirely by the *prior distributions*, is called the **prior predictive distribution**
  + generating prior predictive distributions repeatedly helps us to check whether the priors make sense; we want to know whether the priors generate realistic-looking data
  
- to do this, repeat the following many times:
  1. Tae one sample from each of the priors
  2. Plug those samples into the porbability density/mass function used as the likelihood int he model to generate a dataset $y_{pred_1}, ..., y_{pred_n}$
  - each sample is an imaginary or potential data set

- create a function that does this:

```{r}
normal_predictive_distribution <-
  function(mu_samples, sigma_samples, N_obs) {
    # empty data frame with headers:
    df_pred <- tibble(
      trialn = numeric(0),
      rt_pred = numeric(0),
      iter = numeric(0)
    )
    # i iterates from 1 to the length of mu_samples,
    # which we assume is identical to
    # the length of the sigma_samples:
    for (i in seq_along(mu_samples)) {
      mu <- mu_samples[i]
      sigma <- sigma_samples[i]
      df_pred <- bind_rows(
        df_pred,
        tibble(
          trialn = seq_len(N_obs), # 1, 2,... N_obs
          rt_pred = rnorm(N_obs, mu, sigma),
          iter = i
        )
      )
    }
    df_pred
  }
```

- the code below produces 1000 samples of the prior predictive distribution of the model we defined for `fit_press` from the `df_spacebar` data, that had 361 trials
  + this code will produce 361,000 predicted values (361 observations x 1000 simulations)
  - we could also use the option `sample_prior = "only"` in our `brms` model, but it still depends on Stam's sampler which uses Hamiltonian Monte Carlo, and can fail to converge especially with uninformative priors

```{r}
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 60000)
sigma_samples <- runif(N_samples, 0, 2000)
tic()
prior_pred <- normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
)
toc()
```

```{r}
prior_pred
```

:::{.callout-tip}
## Box 3.1: A more efficint prior predictive distribution function
- alternatively, we could use the `purr::map2_dfr()` functions as below, which would run a bit faster:

```{r, eval = F}
library(purrr)
# Define the function:
normal_predictive_distribution <- function(mu_samples,
                                           sigma_samples,
                                           N_obs) {
  map2_dfr(mu_samples, sigma_samples, function(mu, sigma) {
    tibble(
      trialn = seq_len(N_obs),
      rt_pred = rnorm(N_obs, mu, sigma)
    )
  }, .id = "iter") %>%
    # .id is always a string and
    # needs to be converted to a number
    mutate(iter = as.numeric(iter))
}
# Test the timing:
tic()
prior_pred <- normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
)
toc()
```
:::

- let's look at the first 18 samples of the *prior predictive distribution*

```{r}
#| fig-height: 6
#| label: fig-samples18
#| fig-cap: "18 samples"

prior_pred %>%
  filter(iter <= 18) %>%
  ggplot(aes(rt_pred)) +
  labs(title = "18 samples",
      x="predicted rt (ms)") +
  geom_histogram(aes(y = ..density..)) +
  
  theme(
    axis.text.x =
      element_text(angle = 40, vjust = 1, hjust = 1, size = 14)
  ) +
  scale_y_continuous(
    limits = c(0, 0.0005),
    breaks = c(0, 0.00025, 0.0005), name = "density"
  ) +
  facet_wrap(~iter, ncol = 3)
```

- Figure \@fig-samples18 shows prior data sets that are not realistic: the data shows RT distributions are symmetrical (and we know they are generally right-skewed)
  + worse, a few have *negative* RT values
- so, our priors led to unrealistic values in our prior predictive distribution
  + so our priors weren't very useful
- so, what priors should we have used?

## The influence of priors: sensitivity analysis

- there are 4 main classes of priors in this book
  + but there is no fixed nomenclature for these kind of priors, there's no current naming convention in the field

### Flat, uninformative priors

- the idea behind uninformative priors is to let the data 'speak fo ritself' and to not bias the statistical inference iwth 'subjective priors
- issues with this approach:
  + the prior is as subjective as the likelihood, and different choices of likelihood might have a stronger mpace on the posterior than choice of priors
  + uninformative priors are in general unrealistic and give equal weight to all values within the support of the prior distribution
  + unifnromative priors m ake the sampling slower and might lead to convergence problems
  + it is not always clear which parametrization of a given distribution the flat priors should be assigned to
  
- in our space bar button press example, and uniformative prior would be:

$$
\mu \sim Uniform(-10^{20}, 10^{20})
$$
- this is a strange prior because it's on them millisecond scale, and allows for impossibly large positive values, as well as negative values which are not possible at all

### Regularising priors

- used when we don't have *much* prior information or knowledge
  + sometimes called *weakly informative* or *mildly informative*
- these are priors that down-weight extreme values (they provide *regularization*)
  + usually not very informative, and mostly let the likelihood dominate in determining the posteriors
- these are **theory-neutral** priors; they do not bias the parameters to values spported by any prior belief or theory
- these priors help stabilize computation

- in our button press example, a regularizing prior owuld be

$$
\mu \sim Normal_+(0,1000)
$$
- where $Normal_+$ indicates that the normal distribution is truncated at 0ms (i.e., is cut off at 0, so no negative values are possible)
- this is regularizing because it rules out negative button-pressing times and down-weights extreme values over 2000ms

### Principled priors

- these priors encode all (or most of) the theory-neutral information
  + one generally knows what one's data do and do not look like, it is possible to build priors that truly reflect the properties of potential data sets
- in our button press example, a principled prior could be

$$
\mu \sim Normal_+(250,100)
$$
- it is not overly restrictive, but represents a guess about plausible button-pressing tiems
- *prior predictive checks* using principled priors should produce realisitic distributions of the dependent variable

### Informative priors

- for cases wehre a lot of prior knowledge exists, and not much data
- unless there is a *very* good reason to use informative priors, it is not a good idea to let the priors have too much influence on the posterior
  + e.g., investigating a language-imparied population from which we can't get many subjects, but a lot of previous published work exists on the topic
  
- in our button press data, an informative prior could be based on the meta-analysis of previously published or existing data, or the result of prior elicitation from an expert on the topic under investigation
  + e.g., the following prior:
  
$$
\mu \sim Normal_+(200,20)
$$

- this will have some influence ont he posterior for $\mu$, especially when one has relatively sparse data

## Re-visiting the button-press example with different priors

- what would happen if even wider priors were used for the model we defined earlier?
  + suppose every mean between -10^6 and 10^6 is assumed to be equally likely
  + this is clearly unrealistic and nonsensical; we don't expect negative values
  + for the sd, we could assume any value between 0 and 10^6 is equally likely; the likelihood remains unchanged

$$
\begin{aligned}
\mu &\sim \mathit{Uniform}(-10^{6}, 10^{6}) \\
\sigma &\sim \mathit{Uniform}(0,  10^{6}) 
\end{aligned}
\tag{3.6}
$$

```{r}
# The default settings are used when they are not set explicitly:
# 4 chains, with half of the iterations (set as 3000) as warmup.
fit_press_unif <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(uniform(-10^6, 10^6),
          class = Intercept,
          lb = -10^6,
          ub = 10^6),
    prior(uniform(0, 10^6),
          class = sigma,
          lb = 0,
          ub = 10^6)
  ),
  iter = 3000,
  # the following needed to be changed to achieve convergence
  control = list(adapt_delta = .99,
                 max_treedepth = 15)
)
```

- even with these priors, the output of the model is virtually dientical to the previous one

```{r}
fit_press_unif
```

```{r}
fit_press
```

- what about very informative priors?
  + assume the mean values very close to 400ms are the most likely, and that the sd of RTs is very close to 100ms
  + this is not very sensical, 200ms seems like a more realistic mean for button-press

$$
\begin{aligned}
\mu &\sim \mathit{Normal}(400, 10) \\
\sigma &\sim \mathit{Normal}_+(100, 10) \end{aligned}
\tag{3.7}
$$
- if we refit our model:

```{r}
fit_press_inf <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(normal(400, 10), class = Intercept),
    # brms knows that SDs need to be bounded
    # to exclude values below zero:
    prior(normal(100, 10), class = sigma)
  )
)
```

- we see that the likelihood mostly dominates again, and the new posterior means and CrIs are only shifted by a few milliseconds when these unrealistic but informative priors are used:

```{r}
fit_press_inf
```

- as a final example of sensitivity analysis, let's choose some *principled* priors
- assuming we have some prior experience, let's suppose the mean RT is expected to be arround 200ms, with a 95\% probability of the mean ranging from 0 to 400ms
  + this uncertainty is perhaps unreasonably large, but one might want to allow a bit more uncertainty than one really thinks is reasonable (sometimes called *Cromwell's rules*)
  + let's then decide on the prior $Normal(200,100)$
  + with just a single participnt and a simple task, the residual standard deviation $\sigma$ shouldn't be very large: let's settle on a location of 50ms for a trucnated normal distribution, but still allow for relatively large uncertainty:
  
$$
\begin{aligned}
\mu &\sim \mathit{Normal}(200, 100) \\
\sigma &\sim \mathit{Normal}_+(50, 50) 
\end{aligned}
$$

```{r}
fit_press_prin <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(normal(200, 100), class = Intercept),
    prior(normal(50, 50), class = sigma)
  )
)
```

- again, the estimates are virtually the same as before:

```{r}
fit_press_prin
```

- these examples do not mean priors *never* matter
- when there is enough data, the likelihood will dominate in determing the posterior distributions
  + what constitutes 'enough' data is also a function of the complexity of the model; more complex models require more data, as a rule
- regularized, principled priors (i.e., those that are more consistent with our a priori beliefs about the data) in general speed-up model convergence

- to see how influenced by the priors the posterior is, it's wise to carry out a **sensitivity analysis**: try different priors and either verify that the posterior doesn't chagne drastically, or report how the posterior is affected by some specific priors

## Posterior predictive distribution

- the **posterior predictive distribution** is a *collection of data sets generated from the model* (the likelihood and the priors)
- having obtained the posterior distributions of the parameters after taking into account the data, the posterior distributions can be used to generate future data from the model
  + i.e., given the *posterior distribution* of the parameters of the model, the *posterior* ***predictive*** *distribution* gives us some indication of what future data might look like
  
  - once the posterior distributions $p(\theta|y)$ are available, the predictions based on these distributions, by integrating out the parameters
  
$$
p(\boldsymbol{y_{pred}}\mid \boldsymbol{y} ) = \int_{\boldsymbol{\Theta}} p(\boldsymbol{y_{pred}}, \boldsymbol{\Theta}\mid \boldsymbol{y})\, d\boldsymbol{\Theta}= \int_{\boldsymbol{\Theta}} 
p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta},\boldsymbol{y})p(\boldsymbol{\Theta}\mid \boldsymbol{y})\, d\boldsymbol{\Theta}
$$
- assuming the past and future observations are conditionally independent given $\theta$, the above equation can be written as:

$$
p(\boldsymbol{y_{pred}}\mid \boldsymbol{y} )=\int_{\boldsymbol{\Theta}} p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta}\mid \boldsymbol{y})\, d\boldsymbol{\Theta}
\tag{3.8}
$$

- this **posterior predictive distribution** has important differences from predictions obtained with the *frequentist* approach
  + **frequentist**: gives a point estimate of each predicted observation given the maximum likelihood estimate of $\theta$ (a point value)
  + **Bayesian**: gives a *distribution* of values for each predicated observation

- as with the *prior* predictive distribution, the integration can be carried out computationally by generating samples from the posterior predictive distribution
  + we can use the same function `normal_predictive_distribution()` as created above; the only difference is that the samples come from the **posterior**, not from `mu` and `sigma`
  
```{r}
N_obs <- nrow(df_spacebar)
mu_samples <- as_draws_df(fit_press)$b_Intercept
sigma_samples <- as_draws_df(fit_press)$sigma
normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
)
```
  
- the function `brms::posterior_predict()` is convenient, as it delivers samples from the posterior predictive distribution
  + in a matrix, with the samples as rows and observations (data-points) as columns; so for `fit_press` there'd be 361 columns
  + N.B., if the model is fit with `sample_prior = "only"`, the dependent variable is ignored and `posterior_predict` will give samples from the *prior* predictive distribution

- the **posterior predictive distirubtion** can be used to examine the 'descriptive adequacy' of the model under consideration
  + this is called ***posterior predictive checks***
  + the goal is to establish that the posterior predictive data look more or less similar to the observed data
  + achieveing 'descriptive adequacy' means the current data *could* have been generated by the model
- pass a test of descriptive adequacy is not strong evidence in favour of a model, but a major failure in descriptive adequacy can be interpreted as strong evidence against a model (i.e., passing the test is ***necessary but not sufficient*** evidence in favour of the model)
- in addition, one should check that the *range* of predictions that the model makes is reasonably constrained
  + if a model can capture any possible outcome, then the model fit to a particular data set is not so informative
  + thus, posterior predictive checking is important but only a sanity check to assess whether the model behaviour is reasonable
  
- we can usually just use the plot functions from `brms`
  + e.g., `ppcheck()` takes as arguments the model, number of predicted data sets, and the type of visualisation
    + in these plots, the **observed data** are plotted as $y$, and **predicted data** as $y_{rep}$
    
```{r}
# histograms
pp_check(fit_press, # model
         ndraws = 11, # n of predicted data sets
         type = "hist" # plot type
         )
```
    
    
```{r}
# layered density plots
pp_check(fit_press, # model
         ndraws = 100, # n of predicted data sets
         type = "dens_overlay" # plot type
         )
```

- we see the data ($y$) is slightly skewed and has no values smaller than 100ms, but the predictive distributions are centered and symmetrical
  + so the posterior predictive check shows a slight mismatch between the observed and predicted data
- Can we build a better model? Let's see...

### Comparing different likelihoods

- response times are not usually normally distributed
  + *log-normal* distribution would be more realistic
  
### The log-normal likelihood

- if $y$ is log-normally distributed, that means that $log(y)$ is normally distributed
  + the log-normal distribution is also defined using the parameters location ($\mu$) and scale ($\sigma$), but these are on the log ms scale and correspond to the mean and standard deviation of the logarithm of the data $y$, $log(y)$, which will be normally distributed
  + therefore, when we model some data $y$ using the log-normal likelihood, the parameters $\mu$ and $\sigma$ are on a different scale than the data $y$, which is represented here:
  
$$
\begin{aligned}
\log(\boldsymbol{y}) &\sim \mathit{Normal}( \mu, \sigma)\\
\boldsymbol{y} &\sim \mathit{LogNormal}( \mu, \sigma) 
\end{aligned}
\tag{3.9}
$$

- we can obtain samples from the log-normal distribution, using the normal distribution by first setting an auxiliary variable, $z$, so that $z = log(y)$
  + so, $z \sim Normal(\mu, \sigma)$
  + then we can use $exp(z)$ as samples from the $LogNormal(\mu,\sigma)$
  + since exp($z$) = exp(log($y$)) = $y$
  
```{r}
mu <- 6
sigma <- 0.5
N <- 500000
# Generate N random samples from a log-normal distribution
sl <- rlnorm(N, mu, sigma)
ggplot(tibble(samples = sl), aes(samples)) +
  geom_histogram(aes(y = ..density..), binwidth = 50) +
  ggtitle("Log-normal distribution\n") +
  coord_cartesian(xlim = c(0, 2000))
# Generate N random samples from a normal distribution,
# and then exponentiate them
sn <- exp(rnorm(N, mu, sigma))
ggplot(tibble(samples = sn), aes(samples)) +
  geom_histogram(aes(y = ..density..), binwidth = 50) +
  ggtitle("Exponentiated samples from\na normal distribution") +
  coord_cartesian(xlim = c(0, 2000))
```

### Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood

- if we assume that response times are log-normally distributed, we'll need to change our likelihood function as follows:

$$
rt_n \sim LogNormal(\mu, \sigma)
$$

- but now ***the scale of our priors needs to change***!
  + starting with uniform priors for ease of exposition, although these are really not appropriate:
  
$$
\begin{align}
\mu &\sim Uniform(0,11)\\
\sigma &\sim Uniform(0,1)
\end{align}
$$

- because the parameters are on a different scale than the dependent variable, their interpretation chagnes and it is more complex than dealing with a linear model that assumes a normal likelihood (**location and scale do not coincide with the mean and standard deviation of the log-normal**)
  + ***the location, $\mu$***: in our previous linear model, $\mu$ represented the mean
    + now the mean needs to be calculated in the following way: exp($\frac{\mu + \sigma^2}{2}$)
    + i.e., in the log-normal, the mean is dependent on both $\mu$ and $\sigma$
    + the median is just exp($\mu$)
    + N.B., the prior of $\mu$ is not on the milliseconds scale, but the log milliseconds scale
  + ***the scale, $\sigma$***: the standard deviation of the normal distribution of log($y$)
    + the standard deviation of a log-normal distribution with *location* $\mu$ and *scale* $\sigma$ will be exp($\frac{\mu + \sigma^2}{2} \times \sqrt{exp(\sigma^2) - 1}$)
    + unlike the normal distribution, the spread of the log-normal distribution depends on both $\mu$ and $\sigma$

- to understand the meaning of our priors on the millisecond scale, we need to take into account both the priors and the likelihood; this can be done by generating a **prior predictive distribution**
  + we can just exponentiate the samples produced by `normal_predictive_distribution()`
  
```{r}
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 11)
sigma_samples <- runif(N_samples, 0, 1)
prior_pred_ln <- normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
) %>%
  mutate(rt_pred = exp(rt_pred))
```

- we can't generate negative values anymore (exp(any finite number) > 0)
  + these priors might work in the sense that the model might converge, but it would be better to have **regularizing priors** for the model, such as:
  
$$
\begin{align}
\mu &\sim Normal(6,1.5)\\
\sigma &\sim Normal_+(0,1)
\end{align}
$$

- the prior for $\sigma$ is a truncated distribution
  + although its location is 0, this is not the mean
  + we can calculate its approximate mean from a large number of random samples of the prior distribution using the function `extraDistr::rtnorm()`, where the parameter `a = 0` expresses the fact that the normal distribution is truncated from the left at 0
  
```{r}
mean(rtnorm(100000, # generate n = 100,000
            0, 1,
            a = 0 # truncate at 0
)
)
```

- and even before generating the prior predictive distribution, we can calculate the values within which we are 95\% sure the expected median of the observations will lie
  + we can do this by looking at what happens at 2 standard deviations away from the mean of the prior, $\mu$, that is $6 - 2 \times 1.5$ and $6 + 2 \times 1.5$, and exponentiating these values
  
```{r}
round(c(lower = exp(6 - 2 * 1.5),
        higher = exp(6 + 2 * 1.5)),
      1)
```

- so our prior for $\mu$ is still not too informative (these are medians, the actual values generated by the log-normal distribution can be much more spread out)
  + we can now plot the distribution of some representative statistics of the prior preditive distributions using `brms` to sample from the priors ignoring the `rt` data, by setting `sample_prior = "only"`
  + if we want to use `brms` to generate prior predictive data *before* collecting the data, we do need to have some non-`NA` vlaues as the dependent variable, `rt`
  + setting `sample_prior = "only"` will ignore the data, but we still need to add it: in this case, we add a vector of 1 as "data"
  + we need to specify that the familiy is `lognormal()`

```{r}
# create place-holder data (for cases where we don't yet have any data but want to check out the prior predictive distribution)
df_spacebar_ref <- df_spacebar %>%
  mutate(rt = rep(1, n()))

# now run a model that runs only prior samples
fit_prior_press_ln <- brm(rt ~ 1,
  data = df_spacebar_ref,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma)
  ),
  sample_prior = "only", # this is how we tell the model to only produce priors!
  control = list(adapt_delta = .9)
)
```

- to avoid the warnings, we need to increase the `adapt_delta` parameter's default value from 0.8 to 0.95 to simulate the data

- plot the prior predictive distribution of means with the following code
  + to get a prior predictive distribution, we want to ignore the data, so set `prefix = "ppd"`
  + IMPORTANTLY, this should be run on a model that had `sample_prior = "only"`, and therefore ignored the data; otherwise we'd just be plotting the posterior

```{r}
pp_check(fit_prior_press_ln, type = "stat", stat = "mean", prefix = "ppd") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of means")
```

- to plot the distribution of mimimum and maximum values, replace `mean` with `min` and `max`

```{r}
p1 <- pp_check(fit_prior_press_ln, type = "stat", stat = "mean", prefix = "ppd") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of means")
p2 <- pp_check(fit_prior_press_ln, type = "stat", stat = "min", prefix = "ppd") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of minimum values")
p3 <- pp_check(fit_prior_press_ln, type = "stat", stat = "max", prefix = "ppd") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "10", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of maximum values")
plot_grid(p1, p2, p3, nrow = 3, ncol =1)
```

- these plots show that the priors that we are using are still quite uninformative
  + the tails of the prior predictive distributions that correspond to our normal priors (shown above) are even further to the right, reaching more extreme values than for the prior predictive distributions generated by uniform priors
  + our new priors are still far from representing our prior knowledge
  + we can use summary statistics to test whether the priors are in a plausible range by defining the extreme data that would be very implausible to ever observe
  
```{r}
fit_press_ln <- brm(rt ~ 1,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma)
  )
)
```
  
- when we look at the summary of the posterior, the parameters are on the log-scale

```{r}
fit_press_ln
```

- if we want to know how long it takes to press the space bar in milliseconds, we need to transform the $\mu$ (or `Intercept` in the model) to milliseconds; we know that the median of the log-normal distribution is exp($\mu$), so we do the following to calculate an estimate in milliseconds:

```{r}
estimate_ms <- exp(as_draws_df(fit_press_ln)$b_Intercept)
```

- if we want to know the mean and 95\% CrI of these samples:

```{r}
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
```

- we can now check whether our *predicted data sets* look similar to the observed data

```{r}
pp_check(fit_press_ln, ndraws = 100)
```

- here it seems the posterior predicted data are more similar to the observed data, compared to when we had the normal likelihood
  + but it's not easy to tell
- another way to examine the extent to which the prediced data looks similar to the observed data: look at the distribution of some summary statistics
  + just like with the prior predictive distributions, examine the distribution of representative summary statistics for the data sets generated by different models
  + however, unlike with *prior* predictive distributions, we now have a clear reference: our observed data (which we ignore/don't have yet for prior predictive distributions)
- we suspect that the normal distribution would generate response times that are too fast (since it's symmetrical) and that the log-normal distribution may capture the long tail better than the normal model
  + based on this, we compute the distribution of minimum and maximum values for the posterior predictive distributions, adn compare them with the minimum and maximum values respectively in the data
  + we cn use `pp_check()` to do this, by using as stat `min` or `max` for our models `fit_press` (normal distribution) and `fit_press_ln` (log-normal distribution)
  
```{r}
ggpubr::ggarrange(
  # normal min
  pp_check(fit_press, type = "stat", stat = "min") + 
    labs(title = "Normal model (min)") +
    theme_bw() + theme(legend.position = "none"),
  # normal max
  pp_check(fit_press, type = "stat", stat = "max") + 
    labs(title = "Normal model (max)") +
    theme_bw() + theme(legend.position = "none"),
  # log-normal min
    pp_check(fit_press_ln, type = "stat", stat = "min") + 
    labs(title = "Log-normal model (min)") +
    theme_bw() + theme(legend.position = "none"),
  # log-normal max
  pp_check(fit_press_ln, type = "stat", stat = "max") + 
    labs(title = "Log-normal model (max)") +
    theme_bw() + theme(legend.position = "none"),
  cowplot::get_legend(pp_check(fit_press_ln, type = "stat", stat = "max") + theme(legend.position = "bottom")),
  nrow = 3, ncol = 2, 
  heights = c(.45,.45,.1),
  labels = c("A","B","C","D")
)
```

- here we see the log-normal does a slightly better job since the minimum value is contained in the bulk of the log-normal distribution and in the tai of the normal one

## List of most important commands

- core `brms` function for fitting models, for generating prior predictive and posterior predictive data

```{r}
fit_press <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(uniform(0, 60000), class = Intercept, lb = 0, ub = 60000),
    prior(uniform(0, 2000), class = sigma, lb = 0, ub = 2000)
  ),
  chains = 4,
  iter = 2000,
  warmup = 1000
  ## uncomment for prior predictive:
  ## sample_prior = "only",
  ## uncomment when dealing with divergent transitions
  ## control = list(adapt_delta = .9)
)
```

- extract samples from fitted model:

```{r}
as_draws_df(fit_press)
```

- basic plot of posteriors

```{r}
plot(fit_press)
```

- plot prior predictive/posterior predictive data

```{r}
## Posterior predictive check:
pp_check(fit_press, ndraws = 100, type = "dens_overlay")
## Plot posterior predictive distribution of statistical summaries:
pp_check(fit_press, ndraws = 100, type = "stat", stat = "mean") +
  labs(title = "Posterior predictive distribution")

## Plot prior predictive distribution of statistical summaries:
pp_check(fit_press, ndraws = 100, type = "stat", stat = "mean",
         prefix = "ppd") +
  labs(title = "Prior predictive distribution")
```

## Summary

- in this chapter we:
  + learned how to fit and interpret a Bayesian model with a normal likelihood
  + looked at the effect of priors by means of prior predictive distributions and sensitivity analysis
  + looked at the fit of the posterior by inspecting the posterior predictive distribution (which givees us some idea about the descriptive adequacy of the model)
  + learned how to fit a Bayesian model with a log-normal likelihood, and how to compare the predictive accuracy of different models
  
# Chapter 4 - Bayesian regression models

- regression tells us how our *dependent variable* (response/outcome variable; e.g., pupil size, response times, accuracy, etc.) is affected by one or many *independent variables* (predictors/explanatory variables)
- predictors can be categrocial (e.g., male or female), ordinal (first, second, third, etc.), or continuous (e.g., age)
- this chapter: simple regression with different likelihood functions

## A first linear regression: Does attentional load affect pupil size?

- pupil size is mostly related to the amount of light that reaches the retina or the distance to a perceived object
  + it is *also* systematically influenced by **cognitive processing**: increased cognitive load $\rightarrow$ increase in pupil size
  
- we'll use data from one subject's pupil size of a control experiment in Wahn et al. (2016), *averaged by trial* (`df_pupil` data in the `bcogsci` package)
  + the subject covertly tracks between 0-5 objects among several randomly moving on a screen (multiple object tracking (MOT) task)
  + our goal: investigate how the number of moving objects being tracked (i.e., attentional load) affects pupil size
  
### Likelihood and priors

- we'll model pupil size as normally distributed (we're not expecting skew, and have no further information available about the distribution of pupil sizes)
  + we know this isn't exactly right, given the units used here in which pipil sizes cannot equal 0 or be negative
- for simplicity's sake, let's assume a linear relationship between load and pupil size

Our assumptions:
1. There is some average pupil size, $\alpha$
2. the increase of attentional load has a linear relationship with pupil size, $\beta$
3. There is some noise in this process (variability around true pupil size), represented by the scale $\sigma$

- the generative probability density function will be:

$$
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta,\sigma)
$$
- this translates to the `brms` formula:

```{r, eval = F}
p_size ~ 1 + c_load
```

- where `1` represents the intercept, $\alpha$, which doesn't depend on the predictor
- `c_load` is the predictor that is multiplied by $\beta$
  + the prefix `c_`will be used to indicate a predictor is *centred* (the mean of all the values is subtracted from each value)
  + with load centred, the intercept represents the pupil size at the *average load* in the expeirment (because at the average load, the centered load is zero, yielding $\alpha + 0 \cdot\beta$) 
  + if the load had *not* been centered, then the intercept would represent the pupil size when there is no load

- we could fit a frequentist model simply with `lm(p_size ~ 1 + c_load, data)`, but for a Bayesian model we have to specify priors for each of the parameters

- to set plausible priors, some research into info on pupil sizes needs to first be done
  + e.g., we may know pupil diameters range between 2 to 4mm in bright light to 4-8mm in the dark, this experiment was run with the Eyelink-II eyetracker which uses arbitrary units
  + we need to first know something about measures of pupil sizes (assuming this is our first experiment with such data)
  + luckily we have some *pilot data*, which will tell us something about the order of magnitude of our dependent variable
      + importantly: the pilot data presented no attentional load for the first 100 ms, measured every 10 ms

```{r}
data("df_pupil_pilot")
df_pupil_pilot$p_size %>% summary()
```

- we can now set a regularizing prior for $\alpha$:
  + center the prior around 1000 to be in the right order of magnitude
  + since we don't know much yet about pupil size variation by load, we should include a rather wide prior by defining it as a normal distribution and setting its standard deviation as 500
  
$$
\alpha \sim Normal(1000, 500)
$$
- our predictor load is centered, so with the prior for $\alpha$ we are saying that we suspect that the average pupil size for the average load int he experiment will be in a 95\% CrI of approximately $1000 \pm (2\times500) = [0, 2000]$
  + or, more precisely:

```{r}
round(
  qnorm(c(.025,.975), mean = 1000, sd = 500),
  0)
```

- since the measurements of the pilot data are strongly correlated because they were taken 10 milliseconds apart; so they won't give realistic estimate of variation in pupil size
  + so, set up an uninformative prior for $\sigma$ that encodes this lack of precise information: $\sigma$ is surely longer than zero, and has to be in the order of magnitude of the pupil size with no load
  
$$
\sigma \sim Normal_+(0, 1000)
$$
- so we are saying that we expect that the standard deviation of the pupil sizes should be in the following 95\% CrI:

```{r}
qtnorm(c(.025,.975), 
       mean = 0, 
       sd = 1000, 
       a = 0 # truncate at 0
       )
```

- reminder: the mean of $Normal_+$ does not coincide with its location indicated with $\mu$
  + neither does the standard deviation coincide with the scale $\sigma$
  
```{r}
samples <- rtnorm(20000, 
                  mean = 0, # mean of Normal_+ distri
                  sd = 1000,  # sd of Normal_+ distri
                  a = 0) # truncate at 0
c(mean = mean(samples), # sample mean
  sd = sd(samples)) # sample sd
```
  
- still need the prior for $\beta$, i.e., the change in pupil size produced by attentional load
  + since these changes in pupil size usually aren't perceptible in day-to-day life, they should be much smaller than the pupil size (which we assume ahs a mean of 1000 units), so:
  
$$
\beta \sim Normal(0,100)
$$

- so we are saying we don't really know if the attentional load will increase or decrease the pupil size (it's centered at 0)
  + but we do know one unit of load will potentially change the pupil size in a way that is consistent with the following 95% CrI:
  
```{r}
round(qnorm(c(.025,.975),
      mean = 0,
      sd = 100),0)
```
  
- so, we don't expect changes in size that increase or decrease the pupil size more than 200 units for one unit increase in load

- these priors are relatively uninformative, if we had more previous knowledge about pupil size variation we could have more principled priors

:::{.callout-tip}
## Sidebar: Truncated distributions

- looks intense
:::

### The `brms` model

- before fittings the `brms` model, load the data and center the predictor

```{r}
data("df_pupil")
(df_pupil <- df_pupil %>%
    mutate(c_load = load - mean(load)) # centre
  )
```

- fit the brms model:

```{r}
fit_pupil <- brm(p_size ~ 1 + c_load,
                 data = df_pupil,
                 family = gaussian(),
                 prior = c(
                   prior(normal(1000,500), class = Intercept),
                   prior(normal(0,1000), class = sigma),
                   prior(normal(0,100), 
                         class = b, coef = c_load) # predictor prior
                 )
                 )
```

- if we want to set the same prior for different predictors, we can omit `coef = ...`
- the priors are normal distribution for the intercept ($\alpha$) and the slope ($\beta$), and a truncated normal distribution for the scale parameter $\sigma$, which coincides with the standard deviation (because the likelihood is a normal distribution)

- inspect the output of the model:

```{r}
plot(fit_pupil)
```

```{r}
fit_pupil
```

:::{.callout-tip}
## Sidebar: Intercepts in `brms`

- when setting up a prior for the intercept in `brms`, we actually set a prior for an intercept assuming that all predictors are centered
  + if predictors are *not8 cnetered, there is a mismatch between the interpretation of the intercept as returned in the output and the interpretation of the intercept with respect to its prior specification
:::

### How to communicate the results

- our research question was "What is the effect of attentional load on the subject's pupil size?"
  + we need to examine what happens with the posterior distribution of $\beta$, which is printed out as `c_load` in the model summary, and is `r round(mean(as_draws_df(fit_pupil)$b_c_load),2)`

```{r}
# beta of c_load
round(
  as_draws_df(fit_pupil)$b_c_load %>% 
  mean(),
  2)

# 95% CrI
c(quantile(as_draws_df(fit_pupil)$b_c_load,
           probs = c(.025, .975)))
```

- the model tells us that as attentional load increases, pupil size increases
  + to determine how likely it is that the pupil size increased rather than decreased, we can examine the proportion of samples above zero
  
```{r}
# proportion of cases where the effect occured and was positive
mean(as_draws_df(fit_pupil)$b_c_load > 0)
```
  
- this high probability does not mean the effect of load is non-zero
  + rather, it's much more likely that the effect is positive rather than negative
  + to claim the effect is likely to be non-zero, we'd have to compare the model with an alternative model in which the model assumes that the efefct of load is 0 (more on this in chapter 14)

### Descriptive adequacy

- our model converged and we obtained a posterior distribution
- however there is no guarantee that our model is good enough to represent our data! So let's run some posterior predictive checks
  + it can be useful to customise the PPC to visualise the fit of the model
  
```{r}
for (l in 0:4) { # for the cognitive load levels 0:4
  df_sub_pupil <- filter(df_pupil, load == l) # filter out other loads
  d <- pp_check(fit_pupil, # run pp_check
    type = "dens_overlay",
    ndraws = 100,
    newdata = df_sub_pupil # using this new data with filtering
  ) +
    # and plot it
    geom_point(data = df_sub_pupil, aes(x = p_size, y = 0.0001)) +
    ggtitle(paste("load: ", l)) +
    coord_cartesian(xlim = c(400, 1000))
  # print(p)
  name <- paste0("dens_",l)
  assign(name, d)
}
```

```{r}
ggpubr::ggarrange(
  dens_0 + theme(legend.position = "none"),
  dens_1 + theme(legend.position = "none"),
  dens_2 + theme(legend.position = "none"),
  dens_3 + theme(legend.position = "none"),
  dens_4 + theme(legend.position = "none"),
  cowplot::get_legend(dens_4),
  nrow=2, ncol = 3)
```

- we don't have enough data to make any strong claims; both the predictive distributions and our data are pretty wide spread, and it's hard to tell if the distribution of the observations could've been generated by our model
  + for now let's say it doesn't look so bad

- let's look instead at the distribution of summary statistics

```{r}
  for (l in 0:4) {
  df_sub_pupil <- filter(df_pupil, load == l)
  p <- pp_check(fit_pupil,
    type = "stat",
    ndraws = 1000,
    newdata = df_sub_pupil,
    stat = "mean"
  ) +
    geom_point(data = df_sub_pupil, aes(x = p_size, y = 0.1)) +
    ggtitle(paste("load: ", l)) +
    coord_cartesian(xlim = c(400, 1000))
  # print(p)
  # and store object as p_l
  name <- paste0("p_",l)
  assign(name, p)
  }
```

```{r}
ggpubr::ggarrange(
  p_0 + theme(legend.position = "none"),
  p_1 + theme(legend.position = "none"),
  p_2 + theme(legend.position = "none"),
  p_3 + theme(legend.position = "none"),
  p_4 + theme(legend.position = "none"),
  cowplot::get_legend(p_4),
  nrow=2, ncol = 3)
```

- the observed means for no load and for load = 1 are falling in the tails of the distribution of means
  + the data may be indicating the relevant difference is simply between no load and some load: the likelihood (the dark line) doesn't move much between load = 1:4 (is about 700-740), but is much lower for load=0 (about 550)
  + but given the uncertainty in the posterior predictive distributions and that in the observed means are contained somewhere int he predicted distributions, we could just be overinterpreting noise
  
## Log-normal model: Does trial affect finger tapping times?

- let's revisit the small example from Ch. 3 with the spacebar-pressing times
  + suppose we want ot know whether the subject tended to speed up (a practice effect) or slow down (a fatigue/boredome effect)
  + let's load the data, and centere the column `trial`

```{r}
df_spacebar <- df_spacebar %>%
  mutate(c_trial = trial - mean(trial))
```

### Likelihood and priors for the log-normal model

- if we assume a log-normal distribution of tapping times, the likelihood becomes:

$$
t_n \sim LogNormal(\alpha + c_trial_n \cdot \beta,\sigma)
$$

- where $n = 1,...,N$ and $t$ is the dependent variable (finger tapping times in milliseconds)
  + $N$ is the total number of data points
- same priors as before for $\alpha$ (which is equivalent to $\mu$ in the previous model) and for $\sigma$

$$
\begin{align}
\alpha &\sim Normal(6,1.5)\\
\sigma &\sim Normal(0,1)
\end{align}
$$

- what about a prior for $\beta$?
  + effects are *multiplicative* rather than additive when assuming a log-normal likelihood, meaning we need to take into account $\alpha$ in order to interpret \$beta$
- let's generate some prior predictive distributions to try to understand how our priors interact
  + start with the following prior centred on zero, a prior agnostic regarding the direction of the effect, which allows for a slowdown ($\beta$ > 0) and a speedup ($\beta$ < 0)
  
$$
\beta \sim Normal(0,1)  
$$

- our first attempt at a prior predictive distribution:

```{r}
# Ignore the dependent variable,
# use a vector of ones a placeholder.
df_spacebar_ref <- df_spacebar %>%
  mutate(t = rep(1, n()))
fit_prior_press_trial <- brm(t ~ 1 + c_trial,
  data = df_spacebar_ref,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, 1), class = b, coef = c_trial)
  ),
  sample_prior = "only", # PRIOR PRED DISTR
  control = list(adapt_delta = .9)
)
```

- to understand the type of data we are *assuming* a priori with the prior of parameter $\beta$, let's plot the median difference between the finger tapping times at adjacent trials
  + as the prior of $\beta$ gets wider, larger differences are observed between adjacent trials
- the objective of the prior predictive check is to calibrate the prior of $\beta$ to obtain a plausible range of differences
  + let's plot the distribution of medians because they are less affected by teh variance in the prior predicted distribution than the distribution of mean differences; there'll be more spread for means (the mean of log-normal distributed values depends on both the location, $\mu$, and scale, $\sigma$, of the distribution)

- first, define a function that calculates the difference between adjacent trials, then apply this to the results in `pp_check`
  + as expected, the median effect is centered on zero (as is our prior), but we see that the distribution of possible medians for the effect is too widely spread out and includes extreme values
  
```{r}
# calculate median
median_diff <- function(x) {
  median(x - lag(x), na.rm = TRUE)
}
# plot pp_check for priors
pp_check(fit_prior_press_trial,
         type = "stat",
         stat = "median_diff",
  # show only prior predictive distributions       
         prefix = "ppd",
  # each bin has a width of 500ms       
         binwidth = 500) +
  # cut the top of the plot to improve its scale
  coord_cartesian(ylim = c(0, 50))
```

- try again with the prior $\beta \sim Normal(0,0.01)$
  + this prior predictive distribution still looks quite vague, but is at least in the right order of magnitude

```{r}
# Ignore the dependent variable,
# use a vector of ones a placeholder.
df_spacebar_ref <- df_spacebar %>%
  mutate(t = rep(1, n()))
fit_prior_press_trial <- brm(t ~ 1 + c_trial,
  data = df_spacebar_ref,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, .01), class = b, coef = c_trial) # change to 0,.01
  ),
  sample_prior = "only", # PRIOR PRED DISTR
  control = list(adapt_delta = .9)
)

# calculate median
median_diff <- function(x) {
  median(x - lag(x), na.rm = TRUE)
}
# plot pp_check for priors
pp_check(fit_prior_press_trial,
         type = "stat",
         stat = "median_diff",
  # show only prior predictive distributions       
         prefix = "ppd",
  # each bin has a width of 500ms       
         binwidth = 500) +
  # cut the top of the plot to improve its scale
  coord_cartesian(ylim = c(0, 50))
```

- prior selection is a lot of work, but is usually only done the first time using a certain experimental design
  + and likelihood estimates from frequentist models can be useful too
  + when in doubt, a sensitivity analysis is helpful to determine whether the posterior distribution depends too strongly on our prior
  
### The `brms` model

- let's stick to the priors we've set, and fit the model of the effect of trial using `brms`
  + we need to specify the family is `lognormal()`
  
```{r}
fit_press_trial <- brm(rt ~ 1 + c_trial,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, .01), class = b, coef = c_trial)
  )
)
```

- we can look at the estimates from the posteriors for parameters $\alpha$, $\beta$, and $\sigma$
  + N.B., these are on the log scale

```{r}
# print just the estimates (NB, log scale)
posterior_summary(fit_press_trial,
                  variable = c("b_Intercept",
                               "b_c_trial",
                               "sigma"))
```

- let's plot the **posterior** distributions

```{r}
plot(fit_press_trial)
```

```{r}
# layered density plots
pp_check(fit_press_trial, # model
         ndraws = 100, # n of predicted data sets
         type = "dens_overlay" # plot type
         ) +
  theme_bw()
```

### How toc ommunicate the results

- the first step is to summarize the posteriors in a table or graphically (or both)
  + if the research relates to the effect estimated by the model, the posterior of $\beta$ can be summarised as: $\hat\beta$ = 0.00052, 95\% = [0.0004, 0.00065].
  
- the effect is easier to interpret in milliseconds, so we can back-transform it from the log scale
  + *but* we need to take into account that the scale is not linear, and that the effect between two button presses will differ depending on where we are in the experiment
  
- we will have a certain estimate if we consider the difference between RTs in a trial at the middle of the experiment (when centered trial number is 0) and the previous trial (when the centered trial number is minus 1)
  
```{r}
alpha_samples <- as_draws_df(fit_press_trial)$b_Intercept
beta_samples <- as_draws_df(fit_press_trial)$b_c_trial
effect_middle_ms <- exp(alpha_samples) -
  exp(alpha_samples - 1 * beta_samples)

## ms effect in the middle of the expt
## (mean trial vs. mean trial - 1)
c(mean = mean(effect_middle_ms),
  quantile(effect_middle_ms, c(0.025, 0.975)))
```

- we will obtain different estimates if we consider the difference between the second and the first trial:

```{r}
first_trial <- min(df_spacebar$c_trial) # grab number of first trial
second_trial <- min(df_spacebar$c_trial) + 1 # and the number of the first+1 trial (2nd trial)
effect_beginning_ms <-
  exp(alpha_samples + second_trial * beta_samples) -
  exp(alpha_samples + first_trial * beta_samples)
## ms effect from first to second trial:
c(mean = mean(effect_beginning_ms),
  quantile(effect_beginning_ms, c(0.025, 0.975)))
```

- so far we've obtained ***median*** effects (using `exp()`)
  + if we want ***mean*** effects we need to take $\sigma$ into account, since we need to calculate $\exp(\cdot + \sigma^2/2)$
  + luckily we can just use the built-in function `fitted()` which calculates means
  
- first: define for which observations we want to obtain the fitted values *in milliseconds*
  + we're interested in the 1st and 2nd trials, so create a new data frame with their centred versions

```{r}
newdata_1 <- data.frame(c_trial = c(first_trial, second_trial))
```

- now, use `fitted()` on the `brms object`, including the new data, and set `summary` parameter to `FALSE`
  + first column contains the posterior samples transformed into milliseconds of the first tiral, and the 2nd column for the 2nd trial
  
```{r}
beginning <- fitted(fit_press_trial,
                 newdata = newdata_1,
                 summary = FALSE)
head(beginning, 3)
```

- last, calculate the difference between trials and report mean and 95\% quantiles

```{r}
# calculate *differences* (i.e., effect) between the first 2 trial means
effect_beginning_ms <- beginning[, 2] - beginning[,1]
c(mean = mean(effect_beginning_ms),
  quantile(effect_beginning_ms, c(0.025, 0.975)))
```

- since $\sigma$ is much smaller than $\mu$, $\sigma$ doesn't have a large influence on the mean effects, and the mean and 95\% CrI of the mean and median effects are quite similar

- we see that no matter how we calculate the trial effect there is a *slow down* (estimates are positive)
- when reporting results, present the posterior mean and a CrI and reason about whether the obsrved estimates are consistent with the preiction from the theory being investigated

- the practical relevance of the effect for the RQ can be important too
  + e.g., we only see a barely noticeable slowdown after 100 button presses:
  
```{r}
effect_100 <-
  exp(alpha_samples + 100 * beta_samples) -
  exp(alpha_samples)
c(mean = mean(effect_100),
  quantile(effect_100, c(0.025, 0.975)))
```

- we need to consider: does our uncertainty of this estimate, and the estimated mean effect, have any scientific relevance?
  + considering previous lit, predictions from a quantitative model, other experiment domain knowledge
  
- sometimes we're only interested in establishing whether an effect is present or absent
  + the magnitude and uncertainty in these instances are of secondary interest
  + here the goal is to argue that there is evidence of a slow down
  + in *NHST*, a likelihood ratio test is the standard way to argue for evidence of an effect
  + in *Bayesian data analysis*, we need to carry out **Bayes factor analysis** (see chapters 14-16)
  
### Descriptive adequacy

- let's now look at the predictions of the model
  + we know trial effects are very small, so let's example predictions of the model for differences in RTs between 100 button presses
- define a function, `median_diff100()`, that calculates the median difference between a trial $n$ and a trial $n + 100$
  + we'll compare the observed median difference against the range of predicted differences based on the model and the data, rather than only the model as we did for our prior predictions
  
```{r}
median_diff100 <- function(x) median(x - lag(x, 100), na.rm = TRUE)
pp_check(fit_press_trial,
         type = "stat",
         stat = "median_diff100")
```

- from this, we can conclude that model predictions for differences in response times between trials are reasonable (0 is in the outer tail)

## Logistic regression: does set size affect free recall?

- *generalised* linear models (GLMs) are the bread and butter for many researchers
  + we will focus on one special case of GLMs that has wide applciation: logistic regression

- example data set: a study investigating the capacity level of working memory
  + data are a subset of data from Oberauer (2019)
  + each participant presented word lists of varying lengths (2, 4, 6, or 8 elements)
  + asked to recall a word given its position on the list
    - we will focus on the data from on participant
    
- it is well-established that there is a negative correlation of number of items held in WM and performance (i.e., accuracy)
  + we'll investigate this claim
- load the data (`df_recall`) and centre the predictor (`set_size`)
  
```{r}
# load data
data("df_recall")

# centre set size
df_recall <- df_recall %>%
  mutate(c_set_size = set_size - mean(set_size))

# set sizes in the data set:
df_recall$set_size %>%
  unique() %>% sort()
# and now the centred predictor values:
df_recall$c_set_size %>%
  unique() %>% sort()

# trials by set size
df_recall %>%
  group_by(set_size) %>%
  count()

# check out all variables
df_recall %>%
  head()
```

- the column `correct` records the incorrect vs. correct responses with `0` vs. `1`
- the column `c_set_size` records the centred memory set size
- we want to model the trial-by-trial accuracy and examine whether the probability of recalling a word is related to the # of words int he set that the subject needs to remember

### The likelihood for logistic regression model

- recall: Bernoulli likelihood generates a 0 or 1 response with a particular probability $\theta$
  + one can generate simualted data for 10 trials, with a 50% probability  of getting a 1:
  
```{r}
rbern(10, prob = 0.5)
```

- so, we can define each dependent value `correct_n` in the data as being generated from a Bernoulli random variable with the probability of $\theta_n$, where:
  + `n = 1,...,N` indexes the trial
  + $correct_n$ is the dependent variable (0 indicates an incorrect recall, 1 a correct recall)
  + $theta_n$ is the probability of correctly recalling a probe in a given trial $n$
  
$$
correct_n \sim Bernoulli(\theta_n)
\tag{4.10}
$$

- $\theta_n$ is a probability, and as such bounded to be between 0 and 1, so we cannot just fit a regresion model using the normal (or log-normal) likelihood
  + such a model would assume that the data range from $-\infty$ to $+\infty$ (or $0$ to $+\infty$, as truncated cases), this is not appropriate because we can only have 0's and 1's
  
- the GLM framework solves this by defining a *link function* $g(\cdot)$ that connects the linear model to the quantity to be estimated (here, the probabilities $\theta_n$)
  + the link function used for 0,1 responses is called the ***logit link***, and is defined as:
  
$$
\eta_n = g(\theta_n) = \log\left(\frac{\theta_n}{1-\theta_n}\right)
$$

- the term $\frac{\theta_n}{1-\theta_n}$ is called the ***odds***
  _ the logit link function is therefore a ***log-odds***; it maps probability values ranging from [0,1] to real numbers ranging from $-\infty$ to $+\infty$
  + the logit link function, $\eta = g(\theta)$, and the inverse logit $\theta = g^{-1}(\eta)$, known as the *logistic function*, are shown below
  
```{r}
#| fig-height: 4
#| label: fig-logit
x <- seq(0.001, 0.999, by = 0.001)
y <- log(x / (1 - x))
logistic_dat <- data.frame(theta = x, eta = y)

p1 <- qplot(logistic_dat$theta, logistic_dat$eta, geom = "line") + xlab(expression(theta)) + ylab(expression(eta)) + ggtitle("The logit link") +
  annotate("text",
    x = 0.3, y = 4,
    label = expression(paste(eta, "=", g(theta))), parse = TRUE, size = 8
  ) + theme_bw()


p2 <- qplot(logistic_dat$eta, logistic_dat$theta, geom = "line") + xlab(expression(eta)) + ylab(expression(theta)) + ggtitle("The inverse logit link (logistic)") + annotate("text",
  x = -3.5, y = 0.80,
  label = expression(paste(theta, "=", g^-1, (eta))), parse = TRUE, size = 8
) + theme_bw()

ggpubr::ggarrange(p1, p2, ncol = 2, labels = c("A", "B"))
```

- the linear model is now fit not to the 0,1 responses as the dependent variable, but to $\eta_n$, i.e., the *log-odds*, as the dependent variable:

$$
\eta_n = log(\frac{\theta_n}{1 - \theta_n}) = \alpha + \beta \cdot c\_set\_size
$$

- unlike linear modes, the model is defined without residual error terms ($\epsilon$)
  + once $\eta_n$ is estimated, one can solve for the above equation for $\theta_n$ (i.e. we compute the inverse of the logit function and obtain estimates ont he probability scale)
  + this gives the above-mentioned logistic regression function:
  
$$
\theta_n = g^{-1}(\eta_n) = \frac{exp(\eta_n)}{1 + exp(\eta_n)} = \frac{1}{1 + exp(-\eta_n)}
$$

- the last equality in the equation above arises by dividing both the numerator and denominator by $exp(\eta_n)$

- to sum up, the generalised linear model with the logit link fits the following Bernoulli likelihood:

$$
correct_n \sim Bernoulli(\theta_n)
$$

## Priors for the logistic regression

- to decide on priors for $\alpha$ and $\beta$, we need to take into account that these parameters do not represent *probabilities* or *proportions*, but *log-odds* (the x-axis for the inverse logit link plot above)
  + as we see in that plots, the relationship between log-odds (x-axis) and probabilities (y-axis) is not linear
  
- there are two functions in R that implement the logit and inverse logit functions:
  + `qlogis(p)` for the logit function
  + `plogis(x)` for the inverse logit or logistic function

- now let's set priors for $\alpha$ and $\beta$
  + we centred our predictor, so $\alpha$ = the log-odds of correctly recalling one word in a random position for the average set size of five (because $5 = \frac{2+4+6+8}{4}$), which was not a level in the experiment
  + this is a case where the intercept doesn't have a clear interpretation if we leave the prediction uncetered: with non-centered set size, the intercept will be the log-odds of recalling one word in a set of *zero* words, which obviously makes no sense
  
- the prior for $\alpha$ will depend on how difficult the recall task is
_ we could assume that the probability of recalling a word for an averaged set size, $\alpha$ is centered in .5 (50/50 chance) with a great deal of uncertainty
  + the command `qlogis(.5)` tells us that .5 corresponds to 0 in log-odds space
  
```{r}
qlogis(.5)
```
  
- how do we include a great deal of uncertainty? If we look at plot B above of the inverse logit link, we could decide on a standard deviation of 4 in a normal distribution centered in zero:

$$
\alpha \sim Normal(0,4)
$$

- this would cover the vast majority of probabilities (because the range from -4 to +4 includes most probabilities)
- let's plot this prior in log-odds and in probability scale by drawing random samples

```{r}
#| fig-height: 4
samples_logodds <- tibble(alpha = rnorm(100000, 0, 4))
samples_prob <- tibble(p = plogis(rnorm(100000, 0, 4)))

ggpubr::ggarrange(
  ggplot(samples_logodds, aes(alpha)) +
    geom_density() +
    labs(title = "Prior in log-odds (0,4)") +
    theme_bw(),
  ggplot(samples_prob, aes(p)) +
    geom_density() +
    labs(title = "Prior in probability (0,4)") +
    theme_bw(),
  nrow = 1,  ncol = 2,
  labels = c("A","B")
)
```

- this shows that our prior assigns more **probability maxx** to extreme probabilities of recall than to intermediate values
  + clearly this is not what we want!
- we could try several values for standard deviation of the prior until we find a prior that makes sense for us
  + reducing th sd to 1.5 seems to make sense

$$
\alpha \sim Normal(0,1.5)
$$

```{r}
#| fig-height: 4
#| label: fig-ch4-prior1.5
#| fig-cap: Prior for $\alpha \sim Normal(0.1/5)$ in log-odds (A) and in probability space (B)
samples_logodds <- tibble(alpha = rnorm(100000, 0, 1.5))
samples_prob <- tibble(p = plogis(rnorm(100000, 0, 1.5)))

ggpubr::ggarrange(
  ggplot(samples_logodds, aes(alpha)) +
    geom_density() +
    labs(title = "Prior in log-odds (0,1.5)") +
    theme_bw(),
  ggplot(samples_prob, aes(p)) +
    geom_density() +
    labs(title = "Prior in probability (0,1.5)") +
    theme_bw(),
  nrow = 1,  ncol = 2,
  labels = c("A","B")
)
```

- we need to decide now o the prior for $\beta$, the effect in log-odds of increasing the set size
  + we could choose a normal distribution centered on 0, reflecting our lack of any commitment to the direction of the effect @fig-ch4-prior1.5
  + let's test some priors:
  
a. $\beta \sim Normal(0,1)$
b. $\beta \sim Normal(0,.5)$
b. $\beta \sim Normal(0,.1)$
b. $\beta \sim Normal(0,.01)$
b. $\beta \sim Normal(0,.001)$

- more details on how this is done is given in the box in Box 4.4
  + normally you could run a `brms` models using `sample_prior = "only"` and then `predict()`, but the use of Stan's Hamiltonian sampler for sampling from the priors can lead to convergnce problems with uninformative priors (like we're using); a work around is to use the `r*` family of functions (e.g., `rnorm()`, `rbinom()`, etc.) with loops
  
  - we're going to go with the prior with a standard deviation of 0.1, meaning our priors are now:
  
$$
\begin{align}
\alpha \sim Normal(0,1.5) \\
\beta \sim Normal(0,0.1)
\end{align}
$$

## the `brms` model

- we now have the likelihood, link function, and priors, and can fit the model using `brms`
  + specify that the family is `bernoulli()`, and the link is `logit`
  
```{r}
fit_recall <- brm(
  correct ~ 1 + c_set_size,
  data = df_recall,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    prior(normal(0, .1), class = b, coef = c_set_size)
  )
)
```

- now look at the summary of posteriors of each parameter, keeping in mind they're in log-odds

```{r}
posterior_summary(fit_recall,
                  variable = c("b_Intercept", "b_c_set_size"))
```

- if we look at `b_c_set_size`, increasing the set size has a detrimental effect on recall (negative slope), as expected
- plot posteriors:

```{r}
#| fig-cap: Posterior distribution of parameters in model `fit_recall`, with trace plots
#| #| fig-cap-location: top
plot(fit_recall)
```

- now let's see how we can report our results and what conclusions can be drawn

## How to communicate the findings

- this is similar to the situation with the log-normal model in the previous section
- to describe the effect estimated by the model in log-odds space, we summarise the posterior of $\beta$ with: 
  + $\hat\beta = -0.182, 95\% CrI = [-0.343, -0.022]$

- but this is easiser to understand in proportions rather than log-odds, let's try this
  + first, look at the average accuracy for the task:
  
```{r}
alpha_samples <- as_draws_df(fit_recall)$b_Intercept # sample of intercepts
av_accuracy <- plogis(alpha_samples)
```

- now we can compute summary stats for the intercept, i.e., average accuracy for the task at the centre of the cencred predictor (i.e., 5 elements, which wasn't an actual level...)

```{r}
round(
  c(mean = mean(av_accuracy), quantile(av_accuracy, c(0.025, 0.975))),
  3)
```

- like before, to transform the effect of ourm anipulation to a more interpretable scale (here: proportions), we need to take into account that:
  + the scale is *not linear*, and 
  + the effect of increasing the set size depends on the average accuracy, and the set size that we start from

- so, we can use the following calculation to find out the decrease in accuracy in proportions or probability scale:

```{r}
beta_samples <- as_draws_df(fit_recall)$b_c_set_size # slopes for centred set_size

# effect at CENTRED set_size = 0
effect_middle <- plogis(alpha_samples) -
  plogis(alpha_samples - beta_samples)

# summary stats
round(
  c(mean = mean(effect_middle),
  quantile(effect_middle, c(0.025, 0.975))),
  3)
```

- but this is taking the set size of 5 as the 'middle'/'centred' set size, but we didn't even have this level in the uncentred `set_size`
  + better would be to look at the decrease in accuracy from a set size of 2 to 4

```{r}
four <- 4 - mean(df_recall$set_size) # 'centred' value for set size 4
two <- 2 - mean(df_recall$set_size) # 'centred' value for set size 2

# extract differeces from the samples
effect4m2 <-
  plogis(alpha_samples + four * beta_samples) -
  plogis(alpha_samples + two * beta_samples)

# compute stats
round(
  c(mean = mean(effect4m2),
    quantile(effect4m2,
             c(.025,.975))),
  3)
```

- alternatively, we could back-transform to probability scale using the function `fitted()` instead of `plogis`
  + this will work regardless of the type of link function (e.g., probit link)
- we can consider an imaginary observation where the `c_set_size` is 0, and can now use the `summary()` argument in `fitted()`

```{r}
fitted(fit_recall,
       newdata = data.frame(c_set_size = 0),
       summary = TRUE)[,c("Estimate","Q2.5","Q97.5")]
```

- or, if we want to see the difference in accuracy from the average set size minus one to the average set size, and from a set size of two to four, we need to define `newdata` with these set sizes:

```{r}
new_sets <- data.frame(c_set_size = c(0, -1, four, two)) # where 'four' and 'two' are objects defined already in code above

# 'summary = F': grab simulated accuracies, don't summarise
set_sizes = fitted(fit_recall,
                   newdata = new_sets,
                   summary = FALSE)

# print head of collected simulated accuracies
set_sizes %>% 
  head() %>%
  round(3)
```

- and to calculate the appropriate differences (where column 1 = accuracies at average set_size, column 2 = accuracies at average set_size-1, column 3 = at set_size 4, and column 4 = set_size of 2)

```{r}
effect_middle <- set_sizes[,1] - set_sizes[,2]
effect_4m2 <- set_sizes[,3] - set_sizes[,4]
```

- and calculate summaries

```{r}
# average-1
round(
  c(mean = mean(effect_middle),
    quantile(effect_middle,
             c(.025,.975)))
  ,3)

# 4-2
round(
  c(mean = mean(effect_4m2),
    quantile(effect_4m2,
             c(.025,.975)))
  ,3)
```

- notice we get the same values with `fitted()` as when we calculate effects by hand!

## Descriptive adequacy

- we can use posterior distributions to also make predictions for other conditions not presented int he actual experiment, such as set sizes that weren't even tested!
  + we could then carry out another experiment to investigate whether our model was right
- to make predictions for other set sizes, we extend our data set adding rows with set sizes of 3, 5, 7
  + we'll ad 23 trials for each new set size to match the other set sizes in the experiment
  + remember: **we need to center our predictor based on the *original* mean set size**
    + because we want to maintain our interpretation of the intercept!

```{r}
df_recall_ext <- df_recall %>%
  bind_rows(tibble(
    set_size = rep(c(3,5,7), 23),
    c_set_size = set_size -
      mean(df_recall$set_size),
     correct = 0
  ))
```

```{r}
#| label: fig-ch4-distributions
#| fig-cap-location: top
#| fig-cap: distributions of posteror predicted mean accuracies for tested set sizes (2,4,6,8) and untested set sizes (3,5,7). Observed mean accuracy $y$ is only relevant for tested set sizes; "observed" accuracies of untested set sizes are given aas 0, but we see their predicted values
  
# nicer label for the facets:
set_size <- paste("set size", 2:8) %>%
  setNames(-3:3)
pp_check(fit_recall,
  type = "stat_grouped",
  stat = "mean",
  group = "c_set_size",
  newdata = df_recall_ext,
  facet_args = list(
    ncol = 1, scales = "fixed",
    labeller = as_labeller(set_size)
  ),
  binwidth = 0.02
) +
    theme_bw()
```

- notice that the likelihood is set to 0 for our unobserved set size values of 3, 5, and 7
  + this is because we didn't actually *observe* any data for these values, so the likelihood is set to 0
  + however, the posterior predictive distributions are still caluclated, and we could use these as priors for a subsequent replication where we added these values

# Ch. 5 - Bayesian hierarchical models

- **exchangeability** is the Bayesian aalog of "independent and identically distributed" ($iid$)that appears regularly in classical (i.e., frequentist) statistics

## Exchangeability and hierarchical models

- the idea of exchangeability is as follows:
  + we treat each level of a group (e.g., participant id's) as exchangeable, so we can reassign the indices arbitrarily and lose no information
  + we believe this for the observations within each level in the group as well (e.g., trials within a participant)
  + when we include predictors at the level of the observations, these predictors correspond to experimental manipulations (e.g., attentional load, item number, cloze probability, etc) or maybe also at the group level these predictors indicate characteristics of the levels in the group (e.g., memory span, reading level, etc.)
  + the conditional distributions given these explanatory variables would be exchangeable, i.e., our predictors incorporate all the info that is *not* exchangeable
  + this is why *item* number is an appropriate cluster, but not *trial* number: if we re-assign the item numbers there is no difference in the expected affects (it's a *factor*), but *trial* number encodes some information that is useful e.g., in terms of fatigue or practice effects

- even if we aren't interested in specific cluster-level estimates, hierarchical models allow us to *generalise* to the underlying population (subjects, items) from which the clusters in the sampel were drawn

- exchangeability is important in Bayesian stats because of a theorem called the **Representation Theorem**
  + this theorem states that if a sequence of random variables is exchangeable, then the prior distributions on the parameters in a model are a necessary consequence
  + priors are not merely an arbitrary addition to the frequentist modeling approach
  

  
```{r}
system("say -v Moira your script has finished running")
beepr::beep(sound = "mario")
BRRR::skrrrahh_list()
BRRR::skrrrahh("biggie")
```
  
  